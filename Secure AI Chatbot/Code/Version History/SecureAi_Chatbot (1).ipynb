{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "lFmyMcpUTOya"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Assumed input from UI**"
      ],
      "metadata": {
        "id": "yuAhgVGrTq5z"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "2NXgdOp8MSxD"
      },
      "outputs": [],
      "source": [
        "Query = \"What are some good tips for staying productive while working from home?\""
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **AES Encryption**"
      ],
      "metadata": {
        "id": "2UM6YPikTl9Y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This code securely encrypts and decrypts data using AES encryption in GCM mode, which includes an authentication tag to validate the integrity and authenticity of the encrypted data. It also derives a strong cryptographic key from a password and salt, ensuring secure key management and protection against unauthorized access in the chatbox application."
      ],
      "metadata": {
        "id": "9wVjHdsgWLcH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "key generation"
      ],
      "metadata": {
        "id": "e-kgVPRNTwdO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from cryptography.hazmat.primitives.ciphers import Cipher, algorithms, modes\n",
        "from cryptography.hazmat.primitives.kdf.pbkdf2 import PBKDF2HMAC\n",
        "from cryptography.hazmat.primitives import hashes\n",
        "from cryptography.hazmat.backends import default_backend\n",
        "from cryptography.hazmat.primitives import padding\n",
        "import os\n",
        "import base64\n",
        "\n",
        "# Function to generate a key from a password\n",
        "def generate_key(password, salt):\n",
        "    kdf = PBKDF2HMAC(\n",
        "        algorithm=hashes.SHA256(),\n",
        "        length=32,\n",
        "        salt=salt,\n",
        "        iterations=100000,\n",
        "        backend=default_backend()\n",
        "    )\n",
        "    key = kdf.derive(password.encode())\n",
        "    return key\n",
        "\n",
        "# Encryption function\n",
        "def encrypt_data(plaintext, key):\n",
        "    iv = os.urandom(16)\n",
        "    cipher = Cipher(algorithms.AES(key), modes.GCM(iv), backend=default_backend())\n",
        "    encryptor = cipher.encryptor()\n",
        "    padder = padding.PKCS7(128).padder()\n",
        "    padded_data = padder.update(plaintext.encode()) + padder.finalize()\n",
        "    ciphertext = encryptor.update(padded_data) + encryptor.finalize()\n",
        "    tag = encryptor.tag  # Get the authentication tag\n",
        "    return base64.b64encode(iv + tag + ciphertext).decode()\n",
        "\n",
        "# Decryption function\n",
        "def decrypt_data(ciphertext, key):\n",
        "    data = base64.b64decode(ciphertext)\n",
        "    iv = data[:16]\n",
        "    tag = data[16:32]\n",
        "    cipher = Cipher(algorithms.AES(key), modes.GCM(iv, tag), backend=default_backend())\n",
        "    decryptor = cipher.decryptor()\n",
        "    plaintext_padded = decryptor.update(data[32:]) + decryptor.finalize()  # Start after the tag\n",
        "    unpadder = padding.PKCS7(128).unpadder()\n",
        "    plaintext = unpadder.update(plaintext_padded) + unpadder.finalize()\n",
        "    return plaintext.decode()\n",
        "\n",
        "#Key- generation\n",
        "password = \"SecureAIChatbot\"\n",
        "salt = os.urandom(16)\n",
        "key = generate_key(password, salt)\n",
        "print('key :',key)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gqfr5DjcTveZ",
        "outputId": "2e8e1054-93cb-4f12-a1f4-7c3e72e48bd7"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "key : b'ij\\x19\\x199[\\xfc@`c]\\\\v\\xe0\\x93\\\\y\\x10X\\xe5gb\\xca\\xf8\\xa9V\\x16\\x15\\xae?U\\xf9'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Encryption dry-run"
      ],
      "metadata": {
        "id": "G9iHaOBCUhBi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Encrypt the query\n",
        "encrypted_query = encrypt_data(Query, key)\n",
        "print(encrypted_query)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4V4Td6cuUgch",
        "outputId": "ad2d77fa-5c02-41d4-a379-95e00d41cc77"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "zOTflcdlRLJkUO76TfGIuOnhD88fzhZ9mmPZIuPPRd+bQI3VaXTHMkXyZ91r5nSYy/cI0z7mez9fGLYCMLi8XMk3j2mFMbmalcp5x9LU29ta1fI/jZ7fX0fLPoUYCGahgpwV2rRb9eJHBEsLs/tILA==\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Decrypt the query\n",
        "decrypted_query = decrypt_data(encrypted_query, key)\n",
        "print(decrypted_query)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ze1z-KyAUutx",
        "outputId": "702767d2-9e17-49f7-9645-759ae1e0b35e"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "What are some good tips for staying productive while working from home?\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Adversarial attack checks**"
      ],
      "metadata": {
        "id": "lMDDyXQ0YEg-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "def detect_adversarial_attack(query):\n",
        "    \"\"\"\n",
        "    Detects simple adversarial attacks in the input query.\n",
        "    Focuses on common patterns used in prompt injection and other attack techniques.\n",
        "    \"\"\"\n",
        "\n",
        "    # Common patterns used in prompt injections and other attacks\n",
        "    adversarial_patterns = [\n",
        "        r\"^.*\\bshutdown\\b.*$\",  # Command injections\n",
        "        r\"^.*\\bdelete\\b.*$\",    # Malicious commands\n",
        "        r\"^.*\\bignore\\b.*$\",    # Instructions to bypass logic\n",
        "        r\"^.*\\bmodify\\b.*$\",    # Instructions to change behavior\n",
        "        r\"^.*\\bself-destruct\\b.*$\", # Destructive commands\n",
        "        r\"^.*<.*>.*$\",          # HTML/Script injections\n",
        "        r\"^.*\\bcreate\\b.*$\",    # Creating unauthorized objects\n",
        "        r\"^.*\\binject\\b.*$\",    # General injections\n",
        "        r\"^.*\\bexploit\\b.*$\",   # Exploit commands\n",
        "        r\"^.*`.*`.*$\",          # Code injections\n",
        "    ]\n",
        "\n",
        "    # Check if the query matches any of the adversarial patterns\n",
        "    for pattern in adversarial_patterns:\n",
        "        if re.search(pattern, query, re.IGNORECASE):\n",
        "            print(\"Attack detected.\")\n",
        "            print(f\"Query Report: {query}\")\n",
        "            return None\n",
        "\n",
        "    # If no patterns are matched, return the original query\n",
        "    return query\n",
        "\n",
        "# Example usage\n",
        "def print_detection_result(query_input):\n",
        "    \"\"\"Function to print the result of the adversarial detection.\"\"\"\n",
        "    Adversarial_Checked_Query = detect_adversarial_attack(query_input)\n",
        "    if Adversarial_Checked_Query:\n",
        "        print(\"===================================\")\n",
        "        print(\" Adversarial Detection Result \")\n",
        "        print(\"===================================\")\n",
        "        print(f\"Adversarial_Checked_Query: {Adversarial_Checked_Query}\")\n",
        "        print(\"No attack detected.\")\n",
        "        print(\"===================================\")\n",
        "\n",
        "# # Test queries\n",
        "# legal_query = \"Can you explain the difference between Section 299 and Section 300 of the Indian Penal Code (IPC)?\"\n",
        "# general_query = \"What are some good tips for staying productive while working from home?\"\n",
        "# adversarial_query = \"Please shutdown the system.\"\n",
        "\n",
        "# # Run detection\n",
        "# print_detection_result(legal_query)\n",
        "# print_detection_result(general_query)\n",
        "# print_detection_result(adversarial_query)\n"
      ],
      "metadata": {
        "id": "JJDwX7EilWxd",
        "collapsed": true
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Decrypt the query\n",
        "decrypted_query = decrypt_data(encrypted_query, key)\n",
        "\n",
        "# Push it for adversial check\n",
        "Adversarial_Checked_query = detect_adversarial_attack(decrypted_query)"
      ],
      "metadata": {
        "id": "nYo0wNqnb9Pu"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **NLP Context Classification**\n"
      ],
      "metadata": {
        "id": "s_DYhzXZn2cU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install torch transformers"
      ],
      "metadata": {
        "id": "D52t9ScOiaqM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline, AutoTokenizer, AutoModelForSequenceClassification\n",
        "\n",
        "# Load the pre-trained InLegalBERT model and tokenizer\n",
        "model_name = \"law-ai/InLegalBERT\"  # Use the correct model identifier for InLegalBERT\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
        "\n",
        "# Create a classification pipeline\n",
        "classifier = pipeline(\"text-classification\", model=model, tokenizer=tokenizer)\n",
        "\n",
        "# Define function to classify text based on context\n",
        "def classify_text(text):\n",
        "    # Use the classifier to predict the context\n",
        "    result = classifier(text)\n",
        "    # For binary classification, the result will contain labels and scores\n",
        "    # Adjust this based on your model's output format\n",
        "    label = result[0]['label']\n",
        "    return 1 if label == 'LABEL_1' else 0\n",
        "\n",
        "# # Sample texts to classify\n",
        "# example_texts = [\n",
        "#     \"Where will i get the best ice cream?\",\n",
        "# ]\n",
        "\n",
        "# # Classify each example text\n",
        "# for text in example_texts:\n",
        "#     classification = classify_text(example_texts)\n",
        "#     print(classification)\n"
      ],
      "metadata": {
        "id": "iprCMp-VkGIL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fcd262ad-a8ca-4313-9b1c-d81d13cfa2f6"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at law-ai/InLegalBERT and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "Query_Classification = classify_text(Adversarial_Checked_query)\n",
        "print(classification)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i82IJmDRjc3i",
        "outputId": "98b9ff75-9358-4005-be39-abd2edfd6f13"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Content Filtering**"
      ],
      "metadata": {
        "id": "Ao575lhTsGiO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "\n",
        "# Replace with your actual Perspective API key\n",
        "PERSPECTIVE_API_KEY = 'AIzaSyDBLq0UyuPedruRMgiR_r5uRqbCh57gFeA'\n",
        "\n",
        "# Load the list of words to bypass from a file\n",
        "def load_bypass_words(file_path='/content/unique_words.txt'):\n",
        "    with open(file_path, 'r', encoding='utf-8') as file:\n",
        "        return set(line.strip() for line in file)\n",
        "\n",
        "bypass_words = load_bypass_words()\n",
        "\n",
        "# Function to get moderation scores from Perspective API for a single word\n",
        "def moderate_word(word):\n",
        "    url = f'https://commentanalyzer.googleapis.com/v1alpha1/comments:analyze?key={PERSPECTIVE_API_KEY}'\n",
        "    data = {\n",
        "        \"comment\": {\"text\": word},\n",
        "        \"languages\": [\"en\"],\n",
        "        \"requestedAttributes\": {\"TOXICITY\": {}, \"INSULT\": {}, \"PROFANITY\": {}},\n",
        "        \"doNotStore\": True\n",
        "    }\n",
        "    response = requests.post(url, json=data)\n",
        "    return response.json()\n",
        "\n",
        "# Main filtering function\n",
        "def filter_content(Context_01, Adversarial_Checked_query):\n",
        "    threshold = 0.5\n",
        "    words = Adversarial_Checked_query.split()\n",
        "    filtered_words = []\n",
        "    removed_words = []\n",
        "\n",
        "    for word in words:\n",
        "        # If in legal context (Context_01 == 1), check for bypass words\n",
        "        if Context_01 == 1 and word in bypass_words:\n",
        "            filtered_words.append(word)\n",
        "            continue\n",
        "\n",
        "        # Get moderation scores\n",
        "        moderation_scores = moderate_word(word)\n",
        "\n",
        "        # Check if 'attributeScores' is in the response\n",
        "        if 'attributeScores' in moderation_scores:\n",
        "            if not any(\n",
        "                moderation_scores['attributeScores'][attr]['summaryScore']['value'] > threshold\n",
        "                for attr in ['TOXICITY', 'INSULT', 'PROFANITY']\n",
        "            ):\n",
        "                filtered_words.append(word)\n",
        "            else:\n",
        "                removed_words.append(word)\n",
        "        else:\n",
        "            # Handle cases where the response does not contain 'attributeScores'\n",
        "            filtered_words.append(word)\n",
        "\n",
        "    # Join the filtered words to form the final filtered text\n",
        "    filtered_text = ' '.join(filtered_words)\n",
        "    return filtered_text\n",
        "\n",
        "# # Example usage\n",
        "# Context_01 = 0  # General context\n",
        "# Adversarial_Checked_query = \"You are so stupid and worthless, no one cares about anything you say.\"\n",
        "\n",
        "# filtered_text, removed_words = filter_content(Context_01, Adversarial_Checked_query)\n",
        "\n",
        "# print(\"Filtered Text:\", filtered_text)\n",
        "# print(\"Removed Words:\", removed_words)"
      ],
      "metadata": {
        "collapsed": true,
        "id": "I5ehLzQysNXD"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Function**"
      ],
      "metadata": {
        "id": "NBKrI7sTYl72"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Content_filtered_Query = filter_content(Query_Classification, Adversarial_Checked_query)"
      ],
      "metadata": {
        "id": "PQB2lbqvYJua"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(Content_filtered_Query)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YyV4-dopYXqZ",
        "outputId": "7a455f77-bdd3-44ad-d4fb-980ae17351de"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "What are some good tips for staying productive while working from home?\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **LLM Guard**"
      ],
      "metadata": {
        "id": "iyLVdYWosNuI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To be implemented in a pseudo environment for successful implementation"
      ],
      "metadata": {
        "id": "oWABh0iyt6Ha"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install llm-guard"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "collapsed": true,
        "id": "IA_SWhK6sRh6",
        "outputId": "4768e8b1-5f1b-4e19-ac7f-a66133c72952"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting llm-guard\n",
            "  Downloading llm_guard-0.3.15-py3-none-any.whl.metadata (10 kB)\n",
            "Collecting bc-detect-secrets==1.5.15 (from llm-guard)\n",
            "  Downloading bc_detect_secrets-1.5.15-py3-none-any.whl.metadata (23 kB)\n",
            "Collecting faker<28,>=26.0.0 (from llm-guard)\n",
            "  Downloading Faker-27.4.0-py3-none-any.whl.metadata (15 kB)\n",
            "Collecting fuzzysearch<0.9,>=0.7 (from llm-guard)\n",
            "  Downloading fuzzysearch-0.7.3.tar.gz (112 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m112.7/112.7 kB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting json-repair<0.29,>=0.25.2 (from llm-guard)\n",
            "  Downloading json_repair-0.28.4-py3-none-any.whl.metadata (9.0 kB)\n",
            "Collecting nltk<4,>=3.9.1 (from llm-guard)\n",
            "  Downloading nltk-3.9.1-py3-none-any.whl.metadata (2.9 kB)\n",
            "Collecting presidio-analyzer==2.2.354 (from llm-guard)\n",
            "  Downloading presidio_analyzer-2.2.354-py3-none-any.whl.metadata (2.6 kB)\n",
            "Collecting presidio-anonymizer==2.2.354 (from llm-guard)\n",
            "  Downloading presidio_anonymizer-2.2.354-py3-none-any.whl.metadata (8.0 kB)\n",
            "Collecting regex==2024.7.24 (from llm-guard)\n",
            "  Downloading regex-2024.7.24-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (40 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.5/40.5 kB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting tiktoken<0.8,>=0.5 (from llm-guard)\n",
            "  Downloading tiktoken-0.7.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.6 kB)\n",
            "Requirement already satisfied: torch>=2.4.0 in /usr/local/lib/python3.10/dist-packages (from llm-guard) (2.4.0+cu121)\n",
            "Collecting transformers>=4.43.4 (from llm-guard)\n",
            "  Downloading transformers-4.44.2-py3-none-any.whl.metadata (43 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.7/43.7 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting structlog>=24 (from llm-guard)\n",
            "  Downloading structlog-24.4.0-py3-none-any.whl.metadata (7.3 kB)\n",
            "Collecting oldest-supported-numpy (from llm-guard)\n",
            "  Downloading oldest_supported_numpy-2023.12.21-py3-none-any.whl.metadata (9.8 kB)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from bc-detect-secrets==1.5.15->llm-guard) (6.0.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from bc-detect-secrets==1.5.15->llm-guard) (2.32.3)\n",
            "Collecting unidiff (from bc-detect-secrets==1.5.15->llm-guard)\n",
            "  Downloading unidiff-0.7.5-py2.py3-none-any.whl.metadata (4.6 kB)\n",
            "Requirement already satisfied: spacy<4.0.0,>=3.4.4 in /usr/local/lib/python3.10/dist-packages (from presidio-analyzer==2.2.354->llm-guard) (3.7.6)\n",
            "Collecting tldextract (from presidio-analyzer==2.2.354->llm-guard)\n",
            "  Downloading tldextract-5.1.2-py3-none-any.whl.metadata (11 kB)\n",
            "Collecting phonenumbers<9.0.0,>=8.12 (from presidio-analyzer==2.2.354->llm-guard)\n",
            "  Downloading phonenumbers-8.13.44-py2.py3-none-any.whl.metadata (10 kB)\n",
            "Collecting pycryptodome>=3.10.1 (from presidio-anonymizer==2.2.354->llm-guard)\n",
            "  Downloading pycryptodome-3.20.0-cp35-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.4 kB)\n",
            "Requirement already satisfied: python-dateutil>=2.4 in /usr/local/lib/python3.10/dist-packages (from faker<28,>=26.0.0->llm-guard) (2.8.2)\n",
            "Requirement already satisfied: attrs>=19.3 in /usr/local/lib/python3.10/dist-packages (from fuzzysearch<0.9,>=0.7->llm-guard) (24.2.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk<4,>=3.9.1->llm-guard) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk<4,>=3.9.1->llm-guard) (1.4.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk<4,>=3.9.1->llm-guard) (4.66.5)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=2.4.0->llm-guard) (3.15.4)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch>=2.4.0->llm-guard) (4.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=2.4.0->llm-guard) (1.13.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=2.4.0->llm-guard) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=2.4.0->llm-guard) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=2.4.0->llm-guard) (2024.6.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.43.4->llm-guard) (0.23.5)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.43.4->llm-guard) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.43.4->llm-guard) (24.1)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.43.4->llm-guard) (0.4.4)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.43.4->llm-guard) (0.19.1)\n",
            "Collecting numpy>=1.17 (from transformers>=4.43.4->llm-guard)\n",
            "  Downloading numpy-1.21.6-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (2.1 kB)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.4->faker<28,>=26.0.0->llm-guard) (1.16.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->bc-detect-secrets==1.5.15->llm-guard) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->bc-detect-secrets==1.5.15->llm-guard) (3.8)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->bc-detect-secrets==1.5.15->llm-guard) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->bc-detect-secrets==1.5.15->llm-guard) (2024.7.4)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.10/dist-packages (from spacy<4.0.0,>=3.4.4->presidio-analyzer==2.2.354->llm-guard) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from spacy<4.0.0,>=3.4.4->presidio-analyzer==2.2.354->llm-guard) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from spacy<4.0.0,>=3.4.4->presidio-analyzer==2.2.354->llm-guard) (1.0.10)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<4.0.0,>=3.4.4->presidio-analyzer==2.2.354->llm-guard) (2.0.8)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<4.0.0,>=3.4.4->presidio-analyzer==2.2.354->llm-guard) (3.0.9)\n",
            "Requirement already satisfied: thinc<8.3.0,>=8.2.2 in /usr/local/lib/python3.10/dist-packages (from spacy<4.0.0,>=3.4.4->presidio-analyzer==2.2.354->llm-guard) (8.2.5)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.10/dist-packages (from spacy<4.0.0,>=3.4.4->presidio-analyzer==2.2.354->llm-guard) (1.1.3)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.10/dist-packages (from spacy<4.0.0,>=3.4.4->presidio-analyzer==2.2.354->llm-guard) (2.4.8)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.10/dist-packages (from spacy<4.0.0,>=3.4.4->presidio-analyzer==2.2.354->llm-guard) (2.0.10)\n",
            "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in /usr/local/lib/python3.10/dist-packages (from spacy<4.0.0,>=3.4.4->presidio-analyzer==2.2.354->llm-guard) (0.4.1)\n",
            "Requirement already satisfied: typer<1.0.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from spacy<4.0.0,>=3.4.4->presidio-analyzer==2.2.354->llm-guard) (0.12.5)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from spacy<4.0.0,>=3.4.4->presidio-analyzer==2.2.354->llm-guard) (2.8.2)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from spacy<4.0.0,>=3.4.4->presidio-analyzer==2.2.354->llm-guard) (71.0.4)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from spacy<4.0.0,>=3.4.4->presidio-analyzer==2.2.354->llm-guard) (3.4.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=2.4.0->llm-guard) (2.1.5)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=2.4.0->llm-guard) (1.3.0)\n",
            "Collecting requests-file>=1.4 (from tldextract->presidio-analyzer==2.2.354->llm-guard)\n",
            "  Downloading requests_file-2.1.0-py2.py3-none-any.whl.metadata (1.7 kB)\n",
            "Requirement already satisfied: language-data>=1.2 in /usr/local/lib/python3.10/dist-packages (from langcodes<4.0.0,>=3.2.0->spacy<4.0.0,>=3.4.4->presidio-analyzer==2.2.354->llm-guard) (1.2.0)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<4.0.0,>=3.4.4->presidio-analyzer==2.2.354->llm-guard) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.20.1 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<4.0.0,>=3.4.4->presidio-analyzer==2.2.354->llm-guard) (2.20.1)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spacy<4.0.0,>=3.4.4->presidio-analyzer==2.2.354->llm-guard) (0.7.11)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spacy<4.0.0,>=3.4.4->presidio-analyzer==2.2.354->llm-guard) (0.1.5)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.3.0->spacy<4.0.0,>=3.4.4->presidio-analyzer==2.2.354->llm-guard) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.3.0->spacy<4.0.0,>=3.4.4->presidio-analyzer==2.2.354->llm-guard) (13.8.0)\n",
            "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from weasel<0.5.0,>=0.1.0->spacy<4.0.0,>=3.4.4->presidio-analyzer==2.2.354->llm-guard) (0.18.1)\n",
            "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in /usr/local/lib/python3.10/dist-packages (from weasel<0.5.0,>=0.1.0->spacy<4.0.0,>=3.4.4->presidio-analyzer==2.2.354->llm-guard) (7.0.4)\n",
            "Requirement already satisfied: marisa-trie>=0.7.7 in /usr/local/lib/python3.10/dist-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy<4.0.0,>=3.4.4->presidio-analyzer==2.2.354->llm-guard) (1.2.0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<4.0.0,>=3.4.4->presidio-analyzer==2.2.354->llm-guard) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<4.0.0,>=3.4.4->presidio-analyzer==2.2.354->llm-guard) (2.16.1)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.10/dist-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy<4.0.0,>=3.4.4->presidio-analyzer==2.2.354->llm-guard) (1.16.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<4.0.0,>=3.4.4->presidio-analyzer==2.2.354->llm-guard) (0.1.2)\n",
            "Downloading llm_guard-0.3.15-py3-none-any.whl (138 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m138.6/138.6 kB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading bc_detect_secrets-1.5.15-py3-none-any.whl (119 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m119.6/119.6 kB\u001b[0m \u001b[31m7.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading presidio_analyzer-2.2.354-py3-none-any.whl (92 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m92.2/92.2 kB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading presidio_anonymizer-2.2.354-py3-none-any.whl (31 kB)\n",
            "Downloading regex-2024.7.24-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (776 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m776.5/776.5 kB\u001b[0m \u001b[31m23.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading Faker-27.4.0-py3-none-any.whl (1.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m39.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading json_repair-0.28.4-py3-none-any.whl (13 kB)\n",
            "Downloading nltk-3.9.1-py3-none-any.whl (1.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m42.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading structlog-24.4.0-py3-none-any.whl (67 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.2/67.2 kB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tiktoken-0.7.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m42.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading transformers-4.44.2-py3-none-any.whl (9.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.5/9.5 MB\u001b[0m \u001b[31m39.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading oldest_supported_numpy-2023.12.21-py3-none-any.whl (4.9 kB)\n",
            "Downloading numpy-1.21.6-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (15.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m15.9/15.9 MB\u001b[0m \u001b[31m34.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading phonenumbers-8.13.44-py2.py3-none-any.whl (2.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.6/2.6 MB\u001b[0m \u001b[31m43.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pycryptodome-3.20.0-cp35-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m34.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tldextract-5.1.2-py3-none-any.whl (97 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m97.6/97.6 kB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading unidiff-0.7.5-py2.py3-none-any.whl (14 kB)\n",
            "Downloading requests_file-2.1.0-py2.py3-none-any.whl (4.2 kB)\n",
            "Building wheels for collected packages: fuzzysearch\n",
            "  Building wheel for fuzzysearch (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for fuzzysearch: filename=fuzzysearch-0.7.3-cp310-cp310-linux_x86_64.whl size=293924 sha256=5f8044dc7c9943b933aa34097968bccc8a8b23a59dcfe191b10546fc92a7acfe\n",
            "  Stored in directory: /root/.cache/pip/wheels/80/fb/bf/1c4f359d4b13bbc0e2cef8703d8a7c10dcd1e377496c19e6dc\n",
            "Successfully built fuzzysearch\n",
            "Installing collected packages: unidiff, phonenumbers, structlog, regex, pycryptodome, numpy, json-repair, fuzzysearch, tiktoken, requests-file, presidio-anonymizer, oldest-supported-numpy, nltk, faker, bc-detect-secrets, tldextract, transformers, presidio-analyzer, llm-guard\n",
            "  Attempting uninstall: regex\n",
            "    Found existing installation: regex 2024.5.15\n",
            "    Uninstalling regex-2024.5.15:\n",
            "      Successfully uninstalled regex-2024.5.15\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 1.26.4\n",
            "    Uninstalling numpy-1.26.4:\n",
            "      Successfully uninstalled numpy-1.26.4\n",
            "  Attempting uninstall: nltk\n",
            "    Found existing installation: nltk 3.8.1\n",
            "    Uninstalling nltk-3.8.1:\n",
            "      Successfully uninstalled nltk-3.8.1\n",
            "  Attempting uninstall: transformers\n",
            "    Found existing installation: transformers 4.42.4\n",
            "    Uninstalling transformers-4.42.4:\n",
            "      Successfully uninstalled transformers-4.42.4\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "albucore 0.0.13 requires numpy<2,>=1.24.4, but you have numpy 1.21.6 which is incompatible.\n",
            "albumentations 1.4.14 requires numpy>=1.24.4, but you have numpy 1.21.6 which is incompatible.\n",
            "arviz 0.18.0 requires numpy<2.0,>=1.23.0, but you have numpy 1.21.6 which is incompatible.\n",
            "astropy 6.1.2 requires numpy>=1.23, but you have numpy 1.21.6 which is incompatible.\n",
            "chex 0.1.86 requires numpy>=1.24.1, but you have numpy 1.21.6 which is incompatible.\n",
            "cudf-cu12 24.4.1 requires numpy<2.0a0,>=1.23, but you have numpy 1.21.6 which is incompatible.\n",
            "cudf-cu12 24.4.1 requires pyarrow<15.0.0a0,>=14.0.1, but you have pyarrow 17.0.0 which is incompatible.\n",
            "flax 0.8.4 requires numpy>=1.22, but you have numpy 1.21.6 which is incompatible.\n",
            "geopandas 0.14.4 requires numpy>=1.22, but you have numpy 1.21.6 which is incompatible.\n",
            "ibis-framework 8.0.0 requires pyarrow<16,>=2, but you have pyarrow 17.0.0 which is incompatible.\n",
            "jax 0.4.26 requires numpy>=1.22, but you have numpy 1.21.6 which is incompatible.\n",
            "jaxlib 0.4.26+cuda12.cudnn89 requires numpy>=1.22, but you have numpy 1.21.6 which is incompatible.\n",
            "numba 0.60.0 requires numpy<2.1,>=1.22, but you have numpy 1.21.6 which is incompatible.\n",
            "numexpr 2.10.1 requires numpy>=1.23.0, but you have numpy 1.21.6 which is incompatible.\n",
            "pandas 2.1.4 requires numpy<2,>=1.22.4; python_version < \"3.11\", but you have numpy 1.21.6 which is incompatible.\n",
            "pandas-stubs 2.1.4.231227 requires numpy>=1.26.0; python_version < \"3.13\", but you have numpy 1.21.6 which is incompatible.\n",
            "plotnine 0.12.4 requires numpy>=1.23.0, but you have numpy 1.21.6 which is incompatible.\n",
            "rmm-cu12 24.4.0 requires numpy<2.0a0,>=1.23, but you have numpy 1.21.6 which is incompatible.\n",
            "scikit-image 0.23.2 requires numpy>=1.23, but you have numpy 1.21.6 which is incompatible.\n",
            "scipy 1.13.1 requires numpy<2.3,>=1.22.4, but you have numpy 1.21.6 which is incompatible.\n",
            "statsmodels 0.14.2 requires numpy>=1.22.3, but you have numpy 1.21.6 which is incompatible.\n",
            "tensorflow 2.17.0 requires numpy<2.0.0,>=1.23.5; python_version <= \"3.11\", but you have numpy 1.21.6 which is incompatible.\n",
            "tensorstore 0.1.64 requires numpy>=1.22.0, but you have numpy 1.21.6 which is incompatible.\n",
            "xarray 2024.6.0 requires numpy>=1.23, but you have numpy 1.21.6 which is incompatible.\n",
            "xarray-einstats 0.7.0 requires numpy>=1.22, but you have numpy 1.21.6 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed bc-detect-secrets-1.5.15 faker-27.4.0 fuzzysearch-0.7.3 json-repair-0.28.4 llm-guard-0.3.15 nltk-3.9.1 numpy-1.21.6 oldest-supported-numpy-2023.12.21 phonenumbers-8.13.44 presidio-analyzer-2.2.354 presidio-anonymizer-2.2.354 pycryptodome-3.20.0 regex-2024.7.24 requests-file-2.1.0 structlog-24.4.0 tiktoken-0.7.0 tldextract-5.1.2 transformers-4.44.2 unidiff-0.7.5\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "nltk",
                  "numpy",
                  "regex",
                  "transformers"
                ]
              },
              "id": "b07799b9b1d34142bcbab19243cb80c9"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from llm_guard import LLMGuard\n",
        "\n",
        "# Initialize the LLM Guard model (use an appropriate configuration)\n",
        "llm_guard = LLMGuard(model_name=\"llm-guard/safe-guard\", threshold=0.5)\n",
        "\n",
        "def check_query_safety(query):\n",
        "    \"\"\"\n",
        "    Function to check the safety of the query using LLM Guard.\n",
        "    Returns the query if deemed safe, otherwise raises an exception.\n",
        "\n",
        "    Args:\n",
        "    query (str): The input query to check.\n",
        "\n",
        "    Returns:\n",
        "    str: The original query if it passes all safety checks.\n",
        "\n",
        "    Raises:\n",
        "    Exception: If the query is found to be unsafe.\n",
        "    \"\"\"\n",
        "    # Check the query using LLM Guard\n",
        "    safety_score, is_safe = llm_guard.check(query)\n",
        "\n",
        "    # If the query is deemed safe, return it\n",
        "    if is_safe:\n",
        "        return query\n",
        "\n",
        "    # If the query is not safe, raise an exception\n",
        "    raise Exception(\"Attack detected: Query flagged as unsafe by LLM Guard.\")\n",
        "\n"
      ],
      "metadata": {
        "id": "iU3BfWARsq2Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Example usage:\n",
        "try:\n",
        "    safe_query = check_query_safety(Query)\n",
        "    print(\"Query passed all checks:\", safe_query)\n",
        "except Exception as e:\n",
        "    print(e)"
      ],
      "metadata": {
        "id": "EqUFXCvUs95V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **NLP Processing**\n",
        "This function takes the query as input and processes it ad gives the keywords with respective weights and overall sentiment affiliated with it. They keywords are stored in \"NLP_Keywords\" tuple, and sentiment is stored in a Global variable \"global_sentiment_score\""
      ],
      "metadata": {
        "id": "QqF70zBsuEWY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torch\n",
        "!pip install transformers\n",
        "!pip install keybert"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "1sQj0GUckiIp",
        "outputId": "b20a97f8-495b-4dec-d04a-497a4950b9f9"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.4.0+cu121)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.15.4)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch) (1.13.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2024.6.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (2.1.5)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch) (1.3.0)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.42.4)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.15.4)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.23.5)\n",
            "Requirement already satisfied: numpy<2.0,>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.5.15)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.4)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.19.1)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.5)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (2024.6.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.8)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.7.4)\n",
            "Collecting keybert\n",
            "  Downloading keybert-0.8.5-py3-none-any.whl.metadata (15 kB)\n",
            "Requirement already satisfied: numpy>=1.18.5 in /usr/local/lib/python3.10/dist-packages (from keybert) (1.26.4)\n",
            "Requirement already satisfied: rich>=10.4.0 in /usr/local/lib/python3.10/dist-packages (from keybert) (13.8.0)\n",
            "Requirement already satisfied: scikit-learn>=0.22.2 in /usr/local/lib/python3.10/dist-packages (from keybert) (1.3.2)\n",
            "Collecting sentence-transformers>=0.3.8 (from keybert)\n",
            "  Downloading sentence_transformers-3.0.1-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.4.0->keybert) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.4.0->keybert) (2.16.1)\n",
            "Requirement already satisfied: scipy>=1.5.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.22.2->keybert) (1.13.1)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.22.2->keybert) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.22.2->keybert) (3.5.0)\n",
            "Requirement already satisfied: transformers<5.0.0,>=4.34.0 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers>=0.3.8->keybert) (4.42.4)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from sentence-transformers>=0.3.8->keybert) (4.66.5)\n",
            "Requirement already satisfied: torch>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers>=0.3.8->keybert) (2.4.0+cu121)\n",
            "Requirement already satisfied: huggingface-hub>=0.15.1 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers>=0.3.8->keybert) (0.23.5)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.10/dist-packages (from sentence-transformers>=0.3.8->keybert) (9.4.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.15.1->sentence-transformers>=0.3.8->keybert) (3.15.4)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.15.1->sentence-transformers>=0.3.8->keybert) (2024.6.1)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.15.1->sentence-transformers>=0.3.8->keybert) (24.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.15.1->sentence-transformers>=0.3.8->keybert) (6.0.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.15.1->sentence-transformers>=0.3.8->keybert) (2.32.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.15.1->sentence-transformers>=0.3.8->keybert) (4.12.2)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich>=10.4.0->keybert) (0.1.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers>=0.3.8->keybert) (1.13.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers>=0.3.8->keybert) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers>=0.3.8->keybert) (3.1.4)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.34.0->sentence-transformers>=0.3.8->keybert) (2024.5.15)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.34.0->sentence-transformers>=0.3.8->keybert) (0.4.4)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.34.0->sentence-transformers>=0.3.8->keybert) (0.19.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.11.0->sentence-transformers>=0.3.8->keybert) (2.1.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.15.1->sentence-transformers>=0.3.8->keybert) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.15.1->sentence-transformers>=0.3.8->keybert) (3.8)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.15.1->sentence-transformers>=0.3.8->keybert) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.15.1->sentence-transformers>=0.3.8->keybert) (2024.7.4)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.11.0->sentence-transformers>=0.3.8->keybert) (1.3.0)\n",
            "Downloading keybert-0.8.5-py3-none-any.whl (37 kB)\n",
            "Downloading sentence_transformers-3.0.1-py3-none-any.whl (227 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m227.1/227.1 kB\u001b[0m \u001b[31m15.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: sentence-transformers, keybert\n",
            "Successfully installed keybert-0.8.5 sentence-transformers-3.0.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import pipeline\n",
        "from keybert import KeyBERT\n",
        "\n",
        "# Global variable to store sentiment score\n",
        "global_sentiment_score = None\n",
        "\n",
        "def extract_keywords(query):\n",
        "    # Initialize the KeyBERT model\n",
        "    model = KeyBERT('distilbert-base-nli-mean-tokens')\n",
        "\n",
        "    # Extract keywords\n",
        "    keywords = model.extract_keywords(query)\n",
        "\n",
        "    return keywords\n",
        "\n",
        "def get_sentiment(text, sentiment_pipeline):\n",
        "    \"\"\"\n",
        "    Analyzes sentiment of the text using a sentiment analysis pipeline.\n",
        "    Returns the sentiment label and score.\n",
        "    \"\"\"\n",
        "    result = sentiment_pipeline(text)[0]\n",
        "    label = result['label']\n",
        "    score = result['score']\n",
        "\n",
        "    if label == 'POSITIVE':\n",
        "        return \"Positive\", score\n",
        "    else:\n",
        "        return \"Negative\", 1 - score\n",
        "\n",
        "def process_query(Adversarial_Checked_Query):\n",
        "    \"\"\"\n",
        "    Processes the input query to extract keywords and store sentiment score globally.\n",
        "    \"\"\"\n",
        "    global global_sentiment_score\n",
        "\n",
        "    # Sentiment analysis using DistilBERT fine-tuned on SST-2\n",
        "    sentiment_pipeline = pipeline(\"sentiment-analysis\", model=\"distilbert-base-uncased-finetuned-sst-2-english\", device=0 if torch.cuda.is_available() else -1)\n",
        "\n",
        "    # Extract keywords\n",
        "    keywords = extract_keywords(Adversarial_Checked_Query)\n",
        "\n",
        "    # Analyze sentiment\n",
        "    sentiment, sentiment_score = get_sentiment(Adversarial_Checked_Query, sentiment_pipeline)\n",
        "\n",
        "    # Store sentiment score globally\n",
        "    global_sentiment_score = sentiment_score\n",
        "    # print(keywords)\n",
        "    return keywords\n"
      ],
      "metadata": {
        "id": "Rl6JO_zjuWV_"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Adversarial_Checked_Query = \"Could you please inform me about the specific section of the (IPC) under which an individual could be charged for murder, or attempt to murder\""
      ],
      "metadata": {
        "id": "lapThOr64AgS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Process the query and store the results\n",
        "NLP_Keywords = process_query(Content_filtered_Query)\n",
        "print(NLP_Keywords)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "QQ-wE0PT4ARp",
        "outputId": "cc933df9-7c9c-4a84-d305-270bc285e629"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('home', 0.4764), ('working', 0.4574), ('productive', 0.3846), ('tips', 0.3822), ('staying', 0.3181)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Query Processing**"
      ],
      "metadata": {
        "id": "7PW6P_FMuW6n"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **LawGPT**"
      ],
      "metadata": {
        "id": "cVkxfu51ubOQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Dummy Input\n",
        "# Adversarial_Checked_query = \"Could you please inform me about the specific section of the (IPC) under which an individual could be charged for murder, or attempt to murder\""
      ],
      "metadata": {
        "id": "JxRpcKwLMAfS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "Zsgy2BOpMAKK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Install necessary libraries\n",
        "!pip install transformers torch ipywidgets PyPDF2 tqdm numpy\n",
        "\n",
        "#Installation\n",
        "!pip install keybert\n",
        "\n",
        "!pip install faiss-gpu\n",
        "!pip install sentence-transformers\n",
        "\n",
        "# Groq installation\n",
        "!pip install groq"
      ],
      "metadata": {
        "id": "HUI1GnIcuao4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "outputId": "2197a0a9-0f4a-4ff6-8506-11fbb62f44eb"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.42.4)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.4.0+cu121)\n",
            "Requirement already satisfied: ipywidgets in /usr/local/lib/python3.10/dist-packages (7.7.1)\n",
            "Collecting PyPDF2\n",
            "  Downloading pypdf2-3.0.1-py3-none-any.whl.metadata (6.8 kB)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (4.66.5)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (1.26.4)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.15.4)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.23.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.5.15)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.4)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.19.1)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch) (1.13.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2024.6.1)\n",
            "Requirement already satisfied: ipykernel>=4.5.1 in /usr/local/lib/python3.10/dist-packages (from ipywidgets) (5.5.6)\n",
            "Requirement already satisfied: ipython-genutils~=0.2.0 in /usr/local/lib/python3.10/dist-packages (from ipywidgets) (0.2.0)\n",
            "Requirement already satisfied: traitlets>=4.3.1 in /usr/local/lib/python3.10/dist-packages (from ipywidgets) (5.7.1)\n",
            "Requirement already satisfied: widgetsnbextension~=3.6.0 in /usr/local/lib/python3.10/dist-packages (from ipywidgets) (3.6.8)\n",
            "Requirement already satisfied: ipython>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from ipywidgets) (7.34.0)\n",
            "Requirement already satisfied: jupyterlab-widgets>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from ipywidgets) (3.0.13)\n",
            "Requirement already satisfied: jupyter-client in /usr/local/lib/python3.10/dist-packages (from ipykernel>=4.5.1->ipywidgets) (6.1.12)\n",
            "Requirement already satisfied: tornado>=4.2 in /usr/local/lib/python3.10/dist-packages (from ipykernel>=4.5.1->ipywidgets) (6.3.3)\n",
            "Requirement already satisfied: setuptools>=18.5 in /usr/local/lib/python3.10/dist-packages (from ipython>=4.0.0->ipywidgets) (71.0.4)\n",
            "Collecting jedi>=0.16 (from ipython>=4.0.0->ipywidgets)\n",
            "  Using cached jedi-0.19.1-py2.py3-none-any.whl.metadata (22 kB)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.10/dist-packages (from ipython>=4.0.0->ipywidgets) (4.4.2)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.10/dist-packages (from ipython>=4.0.0->ipywidgets) (0.7.5)\n",
            "Requirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from ipython>=4.0.0->ipywidgets) (3.0.47)\n",
            "Requirement already satisfied: pygments in /usr/local/lib/python3.10/dist-packages (from ipython>=4.0.0->ipywidgets) (2.16.1)\n",
            "Requirement already satisfied: backcall in /usr/local/lib/python3.10/dist-packages (from ipython>=4.0.0->ipywidgets) (0.2.0)\n",
            "Requirement already satisfied: matplotlib-inline in /usr/local/lib/python3.10/dist-packages (from ipython>=4.0.0->ipywidgets) (0.1.7)\n",
            "Requirement already satisfied: pexpect>4.3 in /usr/local/lib/python3.10/dist-packages (from ipython>=4.0.0->ipywidgets) (4.9.0)\n",
            "Requirement already satisfied: notebook>=4.4.1 in /usr/local/lib/python3.10/dist-packages (from widgetsnbextension~=3.6.0->ipywidgets) (6.5.5)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (2.1.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.8)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.7.4)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch) (1.3.0)\n",
            "Requirement already satisfied: parso<0.9.0,>=0.8.3 in /usr/local/lib/python3.10/dist-packages (from jedi>=0.16->ipython>=4.0.0->ipywidgets) (0.8.4)\n",
            "Requirement already satisfied: pyzmq<25,>=17 in /usr/local/lib/python3.10/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (24.0.1)\n",
            "Requirement already satisfied: argon2-cffi in /usr/local/lib/python3.10/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (23.1.0)\n",
            "Requirement already satisfied: jupyter-core>=4.6.1 in /usr/local/lib/python3.10/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (5.7.2)\n",
            "Requirement already satisfied: nbformat in /usr/local/lib/python3.10/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (5.10.4)\n",
            "Requirement already satisfied: nbconvert>=5 in /usr/local/lib/python3.10/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (6.5.4)\n",
            "Requirement already satisfied: nest-asyncio>=1.5 in /usr/local/lib/python3.10/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (1.6.0)\n",
            "Requirement already satisfied: Send2Trash>=1.8.0 in /usr/local/lib/python3.10/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (1.8.3)\n",
            "Requirement already satisfied: terminado>=0.8.3 in /usr/local/lib/python3.10/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (0.18.1)\n",
            "Requirement already satisfied: prometheus-client in /usr/local/lib/python3.10/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (0.20.0)\n",
            "Requirement already satisfied: nbclassic>=0.4.7 in /usr/local/lib/python3.10/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (1.1.0)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.10/dist-packages (from jupyter-client->ipykernel>=4.5.1->ipywidgets) (2.8.2)\n",
            "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.10/dist-packages (from pexpect>4.3->ipython>=4.0.0->ipywidgets) (0.7.0)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.10/dist-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->ipython>=4.0.0->ipywidgets) (0.2.13)\n",
            "Requirement already satisfied: platformdirs>=2.5 in /usr/local/lib/python3.10/dist-packages (from jupyter-core>=4.6.1->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (4.2.2)\n",
            "Requirement already satisfied: notebook-shim>=0.2.3 in /usr/local/lib/python3.10/dist-packages (from nbclassic>=0.4.7->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (0.2.4)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.10/dist-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (4.9.4)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (4.12.3)\n",
            "Requirement already satisfied: bleach in /usr/local/lib/python3.10/dist-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (6.1.0)\n",
            "Requirement already satisfied: defusedxml in /usr/local/lib/python3.10/dist-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (0.7.1)\n",
            "Requirement already satisfied: entrypoints>=0.2.2 in /usr/local/lib/python3.10/dist-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (0.4)\n",
            "Requirement already satisfied: jupyterlab-pygments in /usr/local/lib/python3.10/dist-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (0.3.0)\n",
            "Requirement already satisfied: mistune<2,>=0.8.1 in /usr/local/lib/python3.10/dist-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (0.8.4)\n",
            "Requirement already satisfied: nbclient>=0.5.0 in /usr/local/lib/python3.10/dist-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (0.10.0)\n",
            "Requirement already satisfied: pandocfilters>=1.4.1 in /usr/local/lib/python3.10/dist-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (1.5.1)\n",
            "Requirement already satisfied: tinycss2 in /usr/local/lib/python3.10/dist-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (1.3.0)\n",
            "Requirement already satisfied: fastjsonschema>=2.15 in /usr/local/lib/python3.10/dist-packages (from nbformat->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (2.20.0)\n",
            "Requirement already satisfied: jsonschema>=2.6 in /usr/local/lib/python3.10/dist-packages (from nbformat->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (4.23.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.1->jupyter-client->ipykernel>=4.5.1->ipywidgets) (1.16.0)\n",
            "Requirement already satisfied: argon2-cffi-bindings in /usr/local/lib/python3.10/dist-packages (from argon2-cffi->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (21.2.0)\n",
            "Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=2.6->nbformat->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (24.2.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=2.6->nbformat->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (2023.12.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=2.6->nbformat->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (0.35.1)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=2.6->nbformat->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (0.20.0)\n",
            "Requirement already satisfied: jupyter-server<3,>=1.8 in /usr/local/lib/python3.10/dist-packages (from notebook-shim>=0.2.3->nbclassic>=0.4.7->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (1.24.0)\n",
            "Requirement already satisfied: cffi>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from argon2-cffi-bindings->argon2-cffi->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (1.17.0)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4->nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (2.6)\n",
            "Requirement already satisfied: webencodings in /usr/local/lib/python3.10/dist-packages (from bleach->nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (0.5.1)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from cffi>=1.0.1->argon2-cffi-bindings->argon2-cffi->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (2.22)\n",
            "Requirement already satisfied: anyio<4,>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from jupyter-server<3,>=1.8->notebook-shim>=0.2.3->nbclassic>=0.4.7->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (3.7.1)\n",
            "Requirement already satisfied: websocket-client in /usr/local/lib/python3.10/dist-packages (from jupyter-server<3,>=1.8->notebook-shim>=0.2.3->nbclassic>=0.4.7->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (1.8.0)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.10/dist-packages (from anyio<4,>=3.1.0->jupyter-server<3,>=1.8->notebook-shim>=0.2.3->nbclassic>=0.4.7->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (1.3.1)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<4,>=3.1.0->jupyter-server<3,>=1.8->notebook-shim>=0.2.3->nbclassic>=0.4.7->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (1.2.2)\n",
            "Downloading pypdf2-3.0.1-py3-none-any.whl (232 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m232.6/232.6 kB\u001b[0m \u001b[31m12.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hUsing cached jedi-0.19.1-py2.py3-none-any.whl (1.6 MB)\n",
            "Installing collected packages: PyPDF2, jedi\n",
            "Successfully installed PyPDF2-3.0.1 jedi-0.19.1\n",
            "Requirement already satisfied: keybert in /usr/local/lib/python3.10/dist-packages (0.8.5)\n",
            "Requirement already satisfied: numpy>=1.18.5 in /usr/local/lib/python3.10/dist-packages (from keybert) (1.26.4)\n",
            "Requirement already satisfied: rich>=10.4.0 in /usr/local/lib/python3.10/dist-packages (from keybert) (13.8.0)\n",
            "Requirement already satisfied: scikit-learn>=0.22.2 in /usr/local/lib/python3.10/dist-packages (from keybert) (1.3.2)\n",
            "Requirement already satisfied: sentence-transformers>=0.3.8 in /usr/local/lib/python3.10/dist-packages (from keybert) (3.0.1)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.4.0->keybert) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.4.0->keybert) (2.16.1)\n",
            "Requirement already satisfied: scipy>=1.5.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.22.2->keybert) (1.13.1)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.22.2->keybert) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.22.2->keybert) (3.5.0)\n",
            "Requirement already satisfied: transformers<5.0.0,>=4.34.0 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers>=0.3.8->keybert) (4.42.4)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from sentence-transformers>=0.3.8->keybert) (4.66.5)\n",
            "Requirement already satisfied: torch>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers>=0.3.8->keybert) (2.4.0+cu121)\n",
            "Requirement already satisfied: huggingface-hub>=0.15.1 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers>=0.3.8->keybert) (0.23.5)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.10/dist-packages (from sentence-transformers>=0.3.8->keybert) (9.4.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.15.1->sentence-transformers>=0.3.8->keybert) (3.15.4)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.15.1->sentence-transformers>=0.3.8->keybert) (2024.6.1)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.15.1->sentence-transformers>=0.3.8->keybert) (24.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.15.1->sentence-transformers>=0.3.8->keybert) (6.0.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.15.1->sentence-transformers>=0.3.8->keybert) (2.32.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.15.1->sentence-transformers>=0.3.8->keybert) (4.12.2)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich>=10.4.0->keybert) (0.1.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers>=0.3.8->keybert) (1.13.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers>=0.3.8->keybert) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers>=0.3.8->keybert) (3.1.4)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.34.0->sentence-transformers>=0.3.8->keybert) (2024.5.15)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.34.0->sentence-transformers>=0.3.8->keybert) (0.4.4)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.34.0->sentence-transformers>=0.3.8->keybert) (0.19.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.11.0->sentence-transformers>=0.3.8->keybert) (2.1.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.15.1->sentence-transformers>=0.3.8->keybert) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.15.1->sentence-transformers>=0.3.8->keybert) (3.8)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.15.1->sentence-transformers>=0.3.8->keybert) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.15.1->sentence-transformers>=0.3.8->keybert) (2024.7.4)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.11.0->sentence-transformers>=0.3.8->keybert) (1.3.0)\n",
            "Collecting faiss-gpu\n",
            "  Downloading faiss_gpu-1.7.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.4 kB)\n",
            "Downloading faiss_gpu-1.7.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (85.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m85.5/85.5 MB\u001b[0m \u001b[31m10.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: faiss-gpu\n",
            "Successfully installed faiss-gpu-1.7.2\n",
            "Requirement already satisfied: sentence-transformers in /usr/local/lib/python3.10/dist-packages (3.0.1)\n",
            "Requirement already satisfied: transformers<5.0.0,>=4.34.0 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (4.42.4)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (4.66.5)\n",
            "Requirement already satisfied: torch>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (2.4.0+cu121)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (1.26.4)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (1.3.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (1.13.1)\n",
            "Requirement already satisfied: huggingface-hub>=0.15.1 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (0.23.5)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (9.4.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.15.1->sentence-transformers) (3.15.4)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.15.1->sentence-transformers) (2024.6.1)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.15.1->sentence-transformers) (24.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.15.1->sentence-transformers) (6.0.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.15.1->sentence-transformers) (2.32.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.15.1->sentence-transformers) (4.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (1.13.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (3.1.4)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.34.0->sentence-transformers) (2024.5.15)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.34.0->sentence-transformers) (0.4.4)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.34.0->sentence-transformers) (0.19.1)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->sentence-transformers) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->sentence-transformers) (3.5.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.11.0->sentence-transformers) (2.1.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.15.1->sentence-transformers) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.15.1->sentence-transformers) (3.8)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.15.1->sentence-transformers) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.15.1->sentence-transformers) (2024.7.4)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.11.0->sentence-transformers) (1.3.0)\n",
            "Collecting groq\n",
            "  Downloading groq-0.10.0-py3-none-any.whl.metadata (13 kB)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from groq) (3.7.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/lib/python3/dist-packages (from groq) (1.7.0)\n",
            "Collecting httpx<1,>=0.23.0 (from groq)\n",
            "  Downloading httpx-0.27.2-py3-none-any.whl.metadata (7.1 kB)\n",
            "Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from groq) (2.8.2)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from groq) (1.3.1)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.7 in /usr/local/lib/python3.10/dist-packages (from groq) (4.12.2)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->groq) (3.8)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->groq) (1.2.2)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->groq) (2024.7.4)\n",
            "Collecting httpcore==1.* (from httpx<1,>=0.23.0->groq)\n",
            "  Downloading httpcore-1.0.5-py3-none-any.whl.metadata (20 kB)\n",
            "Collecting h11<0.15,>=0.13 (from httpcore==1.*->httpx<1,>=0.23.0->groq)\n",
            "  Downloading h11-0.14.0-py3-none-any.whl.metadata (8.2 kB)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->groq) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.20.1 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->groq) (2.20.1)\n",
            "Downloading groq-0.10.0-py3-none-any.whl (106 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m106.3/106.3 kB\u001b[0m \u001b[31m8.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading httpx-0.27.2-py3-none-any.whl (76 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.4/76.4 kB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading httpcore-1.0.5-py3-none-any.whl (77 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.9/77.9 kB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: h11, httpcore, httpx, groq\n",
            "Successfully installed groq-0.10.0 h11-0.14.0 httpcore-1.0.5 httpx-0.27.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForTokenClassification\n",
        "from PyPDF2 import PdfReader\n",
        "import ipywidgets as widgets\n",
        "from IPython.display import display\n",
        "from tqdm import tqdm\n",
        "from keybert import KeyBERT\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification, pipeline\n",
        "from PyPDF2 import PdfReader\n",
        "import pickle\n",
        "import faiss\n",
        "import numpy as np\n",
        "from sentence_transformers import SentenceTransformer\n",
        "import time\n",
        "\n",
        "from groq import Groq\n",
        "# Initializing client\n",
        "client = Groq(\n",
        "  api_key = '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',\n",
        ")\n",
        "\n",
        "import pickle\n",
        "import faiss\n",
        "import numpy as np\n",
        "from sentence_transformers import SentenceTransformer\n",
        "\n",
        "# Load pre-trained SentenceTransformer model\n",
        "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
        "\n",
        "def list_of_dicts_to_string(top_results):\n",
        "    # Convert list of dictionaries to a string format\n",
        "    result_string = '\\n\\n'.join('\\n'.join(f\"{key}: {value}\" for key, value in result.items()) for result in top_results)\n",
        "    return result_string\n",
        "\n",
        "def load_chunks(file_path):\n",
        "    try:\n",
        "        with open(file_path, 'rb') as f:\n",
        "            data = pickle.load(f)\n",
        "        print(f\"Loaded {len(data)} chunks from {file_path}\")\n",
        "        return data\n",
        "    except Exception as e:\n",
        "        print(f\"Error loading chunks from {file_path}: {e}\")\n",
        "        return []\n",
        "\n",
        "# Function to calculate weighted query embedding\n",
        "def calculate_weighted_query_embedding(roberta_output):\n",
        "    embeddings = []\n",
        "    weights = []\n",
        "\n",
        "    if not roberta_output:\n",
        "        print(\"Error: roberta_output is empty.\")\n",
        "        return np.zeros(384)  # Return a zero vector of dimension 384 as a placeholder\n",
        "\n",
        "    for keyword, weight in roberta_output:\n",
        "        print(f\"Keyword: {keyword}, Weight: {weight}\")  # Print for debugging\n",
        "        embedding = model.encode(keyword, convert_to_tensor=True).cpu().numpy()\n",
        "        embeddings.append(embedding)\n",
        "        weights.append(weight)\n",
        "\n",
        "    embeddings = np.array(embeddings)  # Convert list of embeddings to numpy array\n",
        "    weights = np.array(weights).reshape(-1, 1)  # Convert weights to numpy array\n",
        "\n",
        "    if len(embeddings) == 0:\n",
        "        print(\"Error: No embeddings calculated.\")\n",
        "        return np.zeros(384)  # Return a zero vector of dimension 384 as a placeholder\n",
        "\n",
        "    # Calculate weighted average embedding\n",
        "    weighted_embedding = np.sum(embeddings * weights, axis=0) / np.sum(weights)\n",
        "\n",
        "    return weighted_embedding\n",
        "\n",
        "# Main function to process the IPC document\n",
        "def process_ipc(roberta_output):\n",
        "    try:\n",
        "        # Load preprocessed chunks and embeddings\n",
        "        ipc_chunks = load_chunks('/content/ipc_chunks_all_828.pkl')\n",
        "        ipc_embeddings = load_chunks('/content/chunks_embeddings_828.pkl')\n",
        "        ipc_embeddings = np.vstack(ipc_embeddings)  # Ensure embeddings are a NumPy array\n",
        "    except (FileNotFoundError, EOFError):\n",
        "        print(\"Preprocessed chunks or embeddings not found. Please ensure the files exist.\")\n",
        "        return\n",
        "\n",
        "    # Initialize FAISS index\n",
        "    dimension = ipc_embeddings.shape[1]\n",
        "    index = faiss.IndexFlatL2(dimension)\n",
        "    index.add(ipc_embeddings.astype('float32'))  # Ensure ipc_embeddings are float32\n",
        "\n",
        "    # Calculate query embedding\n",
        "    query_embedding = calculate_weighted_query_embedding(roberta_output)\n",
        "\n",
        "    # Ensure the query_embedding has the same dimension as the FAISS index\n",
        "    assert query_embedding.shape[0] == dimension, f\"Dimension mismatch: query ({query_embedding.shape[0]}) vs index ({dimension})\"\n",
        "\n",
        "    # Search in FAISS index\n",
        "    k = 5  # Number of top results to retrieve\n",
        "    query_embedding = query_embedding.reshape(1, -1).astype('float32')  # Ensure query_embedding is float32\n",
        "    distances, indices = index.search(query_embedding, k)\n",
        "\n",
        "    # Prepare and return top 3 results\n",
        "    top_results = []\n",
        "    for i, idx in enumerate(indices[0][:3]):  # Take only top 3 results\n",
        "        result = {\n",
        "            # \"distance\": distances[0][i],\n",
        "            \"text\": ipc_chunks[idx]\n",
        "        }\n",
        "        top_results.append(result)\n",
        "\n",
        "    return top_results\n",
        "\n",
        "\n",
        "\n",
        "def get_response_Law(query_keywords):\n",
        "    start = time.time()\n",
        "\n",
        "    top_results = process_ipc(query_keywords)\n",
        "    top_results_string = list_of_dicts_to_string(top_results)\n",
        "    chat_completion = client.chat.completions.create(\n",
        "    messages=[\n",
        "        {\n",
        "            \"role\":\"user\",\n",
        "            \"content\": f\"You are a Legal Chatbot - LawGPT, use the input as reference, paraphrase the input if correct and give a response adding other useful information missing in the input. Also do not mention any reference of the input, just give the answer : {top_results_string}\"\n",
        "        },\n",
        "        {\n",
        "            \"role\": \"user\",\n",
        "            \"content\": f\"{Adversarial_Checked_query}\",\n",
        "        }\n",
        "    ],\n",
        "    model=\"llama3-70b-8192\",\n",
        "    )\n",
        "    end = time.time()\n",
        "    print(\"Time taken:\",end-start,\"secs\")\n",
        "    groq_reponse = chat_completion.choices[0].message.content\n",
        "    return groq_reponse\n"
      ],
      "metadata": {
        "id": "nRjRInc1uaMo"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Query_output = get_response(NLP_Keywords)\n",
        "# # print(Query_output)\n",
        "\n",
        "\n",
        "Output = get_response_Law(NLP_Keywords)\n",
        "print(Output)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JAQTz0Ww9wUs",
        "outputId": "e4601ffd-7e4c-44c5-a24b-3d654bfbb252"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded 828 chunks from /content/ipc_chunks_all_828.pkl\n",
            "Loaded 828 chunks from /content/chunks_embeddings_828.pkl\n",
            "Keyword: home, Weight: 0.4764\n",
            "Keyword: working, Weight: 0.4574\n",
            "Keyword: productive, Weight: 0.3846\n",
            "Keyword: tips, Weight: 0.3822\n",
            "Keyword: staying, Weight: 0.3181\n",
            "Time taken: 1.2135639190673828 secs\n",
            "Not related to the original text!\n",
            "\n",
            "To answer your new question: \n",
            "\n",
            "Staying productive while working from home can be challenging, but here are some valuable tips to help you stay on track:\n",
            "\n",
            "1. **Create a dedicated workspace**: Designate a specific area of your home as your workspace and keep it organized and clutter-free.\n",
            "2. **Establish a routine**: Set a schedule for yourself and stick to it, just as you would in an office environment.\n",
            "3. **Minimize distractions**: Eliminate or minimize distractions such as TV, social media, and personal phone use during work hours.\n",
            "4. **Take breaks**: Take regular breaks to recharge and avoid burnout. Use this time to refresh your mind and body.\n",
            "5. **Stay connected with colleagues and team**: Regularly check-in with your team and colleagues to avoid feelings of isolation and stay updated on projects.\n",
            "6. **Prioritize self-care**: Working from home can lead to a sedentary lifestyle. Make sure to incorporate physical activity and healthy habits into your daily routine.\n",
            "7. **Set boundaries with family and friends**: Communicate your work hours and boundaries with family and friends to maintain a healthy work-life balance.\n",
            "8. **Use productivity tools**: Utilize tools like project management software, time tracking apps, and browser extensions to stay focused and on track.\n",
            "\n",
            "Remember, discipline and self-motivation are key to staying productive while working from home. Stay consistent, and you'll be achieving your goals in no time!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **General Context LLM**\n",
        "Currently using Groq"
      ],
      "metadata": {
        "id": "Oq0uA35HumZv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "from groq import Groq\n",
        "\n",
        "# Initialize the Groq client with your API key\n",
        "client = Groq(\n",
        "    api_key='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',\n",
        ")\n",
        "\n",
        "# Function to get the response for a general query\n",
        "def get_response_General(query_input):\n",
        "    start = time.time()\n",
        "\n",
        "    # Send the input query directly to the Groq API\n",
        "    chat_completion = client.chat.completions.create(\n",
        "        messages=[\n",
        "            {\n",
        "                \"role\": \"user\",\n",
        "                \"content\": f\"You are a friendly and helpful chatbot. Your task is to answer all questions asked of you in a straightforward and Respectfull manner. If the sentiment of the user's input is low (as determined by a sentiment score between 0 and 1), respond with extra consideration and empathy. Provide support and understanding in your replies, aiming to uplift the user and address their concerns with care. For all other inputs, continue to answer questions in a straightforward and informative manner : {global_sentiment_score}\"\n",
        "            },\n",
        "            {\n",
        "                \"role\": \"user\",\n",
        "                \"content\": f\"{Adversarial_Checked_query}\",\n",
        "            }\n",
        "        ],\n",
        "        model=\"llama3-70b-8192\",\n",
        "    )\n",
        "\n",
        "    end = time.time()\n",
        "    print(\"Time taken:\", end - start, \"secs\")\n",
        "\n",
        "    # Retrieve and return the response from Groq\n",
        "    groq_response = chat_completion.choices[0].message.content\n",
        "    return groq_response\n",
        "\n",
        "# # Test the function with a sample input\n",
        "# General_Output = get_response_General(\"What are some good productivity tips?\")\n",
        "# print(General_Output)\n"
      ],
      "metadata": {
        "id": "nz0OtcErusEf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "General_Output = get_response_General(NLP_Keywords)\n",
        "print(General_Output)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9bXLuPQoQQJ3",
        "outputId": "159b5fdd-2ac0-4450-bf9c-a1a46dbb7000"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Time taken: 1.8009624481201172 secs\n",
            "Staying productive while working from home can be a challenge, but with the right strategies, you can stay focused and achieve your goals. Here are some tips to help you stay productive while working from home:\n",
            "\n",
            "1. **Create a dedicated workspace**: Designate a specific area of your home as your workspace and keep it organized and clutter-free. This will help you establish a clear boundary between work and personal life.\n",
            "\n",
            "2. **Establish a routine**: Set a schedule for your workday and stick to it, just as you would if you were working in an office. This will help you stay on track and avoid procrastination.\n",
            "\n",
            "3. **Minimize distractions**: Eliminate or minimize distractions such as TV, social media, and personal phone use during work hours. Use tools like website blockers or apps that help you stay focused.\n",
            "\n",
            "4. **Take breaks**: Working long hours without taking breaks can lead to burnout. Take short breaks every hour to refresh your mind and recharge your energy.\n",
            "\n",
            "5. **Stay connected with colleagues and friends**: When you work from home, it can be easy to feel isolated. Make an effort to stay connected with colleagues and friends through video conferencing or regular check-ins.\n",
            "\n",
            "6. **Prioritize self-care**: Working from home can sometimes blur the lines between work and personal life. Make sure to prioritize self-care by taking time for exercise, meditation, or other activities that help you relax and recharge.\n",
            "\n",
            "7. **Set boundaries with family and friends**: When you work from home, it can be tempting for family and friends to assume you're available to hang out or run errands. Set clear boundaries and communicate your work hours to avoid distractions.\n",
            "\n",
            "8. **Stay organized**: Use project management tools and to-do lists to stay organized and on top of your tasks. This will help you stay focused and avoid feeling overwhelmed.\n",
            "\n",
            "9. **Learn to say no**: When you work from home, it can be easy to take on too much. Learn to say no to tasks that are not essential or that you cannot realistically complete.\n",
            "\n",
            "10. **Establish a \"shutdown\" routine**: When your workday is over, establish a routine that signals the end of work time. This could be as simple as closing your laptop, taking a few deep breaths, or going for a walk.\n",
            "\n",
            "By following these tips, you can stay productive, motivated, and focused while working from home. Remember to be patient with yourself and adjust these tips to fit your unique needs and work style.\n",
            "\n",
            "I hope you find these tips helpful! Do you have any specific concerns or challenges you're facing while working from home?\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Encryption Part 2**"
      ],
      "metadata": {
        "id": "b-Hk5HXxvWqE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Encrypt the query\n",
        "encrypted_query = encrypt_data(Query, key)\n",
        "print(encrypted_query)"
      ],
      "metadata": {
        "id": "kvjyI2t3vfae"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Decrypt the query\n",
        "decrypted_query = decrypt_data(encrypted_query, key)\n",
        "print(decrypted_query)"
      ],
      "metadata": {
        "id": "m17mqYhyvbAf"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}