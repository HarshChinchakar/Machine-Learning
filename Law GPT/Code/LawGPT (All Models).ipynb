{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "66Tks0GfVf_w",
        "collapsed": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "13bd3b34-c148-4aaf-dd8b-713a81cd9155"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.41.2)\n",
            "Collecting PyPDF2\n",
            "  Downloading pypdf2-3.0.1-py3-none-any.whl (232 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m232.6/232.6 kB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.15.4)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.23.4)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.25.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.5.15)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.19.1)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.4)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.0->transformers) (2023.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.0->transformers) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.6.2)\n",
            "Installing collected packages: PyPDF2\n",
            "Successfully installed PyPDF2-3.0.1\n"
          ]
        }
      ],
      "source": [
        "# Step 1: Install Dependencies\n",
        "!pip install transformers PyPDF2\n",
        "\n",
        "# Step 2: Import Libraries\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification, pipeline\n",
        "from PyPDF2 import PdfReader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Gvs-Nfi5I4tp",
        "outputId": "3f1df59d-b964-46d0-aedc-1e165f4eb7f8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ryk8VzORZnuH"
      },
      "source": [
        "### **Sequence classifier try**"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Original Un-tuned (Without FAISS)**"
      ],
      "metadata": {
        "id": "CYttE6O3q5hc"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MUmFX8qVehVm"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
        "from PyPDF2 import PdfReader\n",
        "import re\n",
        "\n",
        "# Load tokenizer and model for Legal-BERT\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"nlpaueb/legal-bert-base-uncased\")\n",
        "model = AutoModelForSequenceClassification.from_pretrained(\n",
        "    \"nlpaueb/legal-bert-base-uncased\",\n",
        "    num_labels=1  # Assuming regression for relevance scores\n",
        ")\n",
        "\n",
        "# Check for GPU availability\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model.to(device)\n",
        "\n",
        "# Function to read the IPC PDF and extract text\n",
        "def load_ipc_pdf(file_path):\n",
        "    reader = PdfReader(file_path)\n",
        "    text = \"\"\n",
        "    for page in reader.pages:\n",
        "        text += page.extract_text() + \"\\n\"\n",
        "    return text\n",
        "\n",
        "# Function to split text into manageable chunks\n",
        "def split_text(text, chunk_size=512):\n",
        "    text_chunks = re.split(r'(?<=\\.)\\s', text)  # Split at sentence boundaries\n",
        "    chunks = []\n",
        "    current_chunk = \"\"\n",
        "    for sentence in text_chunks:\n",
        "        if len(current_chunk) + len(sentence) <= chunk_size:\n",
        "            current_chunk += sentence + \" \"\n",
        "        else:\n",
        "            chunks.append(current_chunk.strip())\n",
        "            current_chunk = sentence + \" \"\n",
        "    if current_chunk:\n",
        "        chunks.append(current_chunk.strip())\n",
        "    return chunks\n",
        "\n",
        "# Dummy input (keywords) from a hypothetical RoBERTa model\n",
        "query_keywords = {\n",
        "    \"theft\": 0.9,\n",
        "    \"punishment\": 0.8,\n",
        "    \"Maharashtra\": 0.7\n",
        "}\n",
        "\n",
        "# Function to format input for Legal-BERT\n",
        "def format_input(keywords, text_chunks):\n",
        "    formatted_inputs = []\n",
        "    for chunk in text_chunks:\n",
        "        # Create input by combining keywords and text chunks\n",
        "        input_text = \" \".join(keywords.keys()) + \" \" + chunk\n",
        "        formatted_inputs.append(input_text)\n",
        "    return formatted_inputs\n",
        "\n",
        "# Function to calculate relevance scores\n",
        "def calculate_relevance(formatted_inputs):\n",
        "    relevance_scores = []\n",
        "    for input_text in formatted_inputs:\n",
        "        inputs = tokenizer(input_text, return_tensors='pt', truncation=True, padding=True).to(device)\n",
        "        outputs = model(**inputs)\n",
        "        score = outputs.logits.squeeze().item()\n",
        "        relevance_scores.append(score)\n",
        "    return relevance_scores\n",
        "\n",
        "# Load the IPC text\n",
        "ipc_text = load_ipc_pdf('/content/Indian Penal Code Book.pdf')\n",
        "\n",
        "# Split the IPC text into chunks\n",
        "ipc_chunks = split_text(ipc_text)\n",
        "\n",
        "# Format inputs for Legal-BERT\n",
        "formatted_inputs = format_input(query_keywords, ipc_chunks)\n",
        "\n",
        "# Calculate relevance scores\n",
        "relevance_scores = calculate_relevance(formatted_inputs)\n",
        "\n",
        "# Combine chunks with their scores\n",
        "results = list(zip(ipc_chunks, relevance_scores))\n",
        "\n",
        "# Sort results by relevance score in descending order\n",
        "results.sort(key=lambda x: x[1], reverse=True)\n",
        "\n",
        "# Print top results\n",
        "top_n = 5  # Number of top results to display\n",
        "for i, (chunk, score) in enumerate(results[:top_n]):\n",
        "    print(f\"Result {i+1}:\")\n",
        "    print(f\"Score: {score:.4f}\")\n",
        "    print(f\"Text: {chunk}\\n\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s0KbPC3yJuvU",
        "outputId": "da7b986b-fe3c-47af-e561-5b5973bcafff",
        "collapsed": true
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting faiss-gpu\n",
            "  Downloading faiss_gpu-1.7.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (85.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m85.5/85.5 MB\u001b[0m \u001b[31m10.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: faiss-gpu\n",
            "Successfully installed faiss-gpu-1.7.2\n"
          ]
        }
      ],
      "source": [
        "!pip install faiss-gpu"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install torch transformers PyPDF2 tqdm numpy"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "xuvLVsJ4zndY",
        "outputId": "d31d62db-38dd-410a-b0d2-124b3e0eb03c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.3.0+cu121)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.41.2)\n",
            "Requirement already satisfied: PyPDF2 in /usr/local/lib/python3.10/dist-packages (3.0.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (4.66.4)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (1.25.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.15.4)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch) (1.12.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2023.6.0)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch)\n",
            "  Using cached nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.1.105 (from torch)\n",
            "  Using cached nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.1.105 (from torch)\n",
            "  Using cached nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
            "Collecting nvidia-cudnn-cu12==8.9.2.26 (from torch)\n",
            "  Using cached nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n",
            "Collecting nvidia-cublas-cu12==12.1.3.1 (from torch)\n",
            "  Using cached nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
            "Collecting nvidia-cufft-cu12==11.0.2.54 (from torch)\n",
            "  Using cached nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
            "Collecting nvidia-curand-cu12==10.3.2.106 (from torch)\n",
            "  Using cached nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
            "Collecting nvidia-cusolver-cu12==11.4.5.107 (from torch)\n",
            "  Using cached nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
            "Collecting nvidia-cusparse-cu12==12.1.0.106 (from torch)\n",
            "  Using cached nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
            "Collecting nvidia-nccl-cu12==2.20.5 (from torch)\n",
            "  Using cached nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl (176.2 MB)\n",
            "Collecting nvidia-nvtx-cu12==12.1.105 (from torch)\n",
            "  Using cached nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
            "Requirement already satisfied: triton==2.3.0 in /usr/local/lib/python3.10/dist-packages (from torch) (2.3.0)\n",
            "Collecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.5.82-py3-none-manylinux2014_x86_64.whl (21.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.3/21.3 MB\u001b[0m \u001b[31m37.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: huggingface-hub<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.23.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.5.15)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.19.1)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (2.1.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.6.2)\n",
            "Requirement already satisfied: mpmath<1.4.0,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch) (1.3.0)\n",
            "Installing collected packages: nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12\n",
            "Successfully installed nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.20.5 nvidia-nvjitlink-cu12-12.5.82 nvidia-nvtx-cu12-12.1.105\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **# Code to create Embeddings**"
      ],
      "metadata": {
        "id": "Mf0kBT6QM1Pj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install sentence-transformers"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "JA0jvb3Lxl3N",
        "outputId": "892355ac-bc77-4a96-961e-b235cb2e7214"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting sentence-transformers\n",
            "  Downloading sentence_transformers-3.0.1-py3-none-any.whl (227 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/227.1 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m92.2/227.1 kB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m227.1/227.1 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: transformers<5.0.0,>=4.34.0 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (4.41.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (4.66.4)\n",
            "Requirement already satisfied: torch>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (2.3.0+cu121)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (1.25.2)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (1.2.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (1.11.4)\n",
            "Requirement already satisfied: huggingface-hub>=0.15.1 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (0.23.4)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (9.4.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.15.1->sentence-transformers) (3.15.4)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.15.1->sentence-transformers) (2023.6.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.15.1->sentence-transformers) (24.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.15.1->sentence-transformers) (6.0.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.15.1->sentence-transformers) (2.31.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.15.1->sentence-transformers) (4.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (1.12.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (3.1.4)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (8.9.2.26)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (12.1.0.106)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.20.5 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (2.20.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (12.1.105)\n",
            "Requirement already satisfied: triton==2.3.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (2.3.0)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.10/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.11.0->sentence-transformers) (12.5.82)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.34.0->sentence-transformers) (2024.5.15)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.34.0->sentence-transformers) (0.19.1)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.34.0->sentence-transformers) (0.4.3)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->sentence-transformers) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->sentence-transformers) (3.5.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.11.0->sentence-transformers) (2.1.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.15.1->sentence-transformers) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.15.1->sentence-transformers) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.15.1->sentence-transformers) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.15.1->sentence-transformers) (2024.6.2)\n",
            "Requirement already satisfied: mpmath<1.4.0,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.11.0->sentence-transformers) (1.3.0)\n",
            "Installing collected packages: sentence-transformers\n",
            "Successfully installed sentence-transformers-3.0.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from sentence_transformers import SentenceTransformer\n",
        "import pickle\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Load pre-trained SentenceTransformer model\n",
        "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
        "\n",
        "# Function to load IPC chunks from a pickle file\n",
        "def load_chunks(file_path):\n",
        "    with open(file_path, 'rb') as f:\n",
        "        return pickle.load(f)\n",
        "\n",
        "# Function to generate embeddings using SentenceTransformer\n",
        "def generate_embeddings(chunks):\n",
        "    embeddings = []\n",
        "    for chunk in tqdm(chunks, desc=\"Generating embeddings\"):\n",
        "        embedding = model.encode(chunk, convert_to_tensor=True).cpu().numpy()\n",
        "        embeddings.append(embedding)\n",
        "    return np.vstack(embeddings)\n",
        "\n",
        "# Main function to process the IPC document and save embeddings\n",
        "def process_ipc(file_path, output_file):\n",
        "    # Load IPC chunks\n",
        "    ipc_chunks = load_chunks(file_path)\n",
        "\n",
        "    # Generate embeddings for the chunks\n",
        "    embeddings = generate_embeddings(ipc_chunks)\n",
        "\n",
        "    # Save embeddings to a pickle file\n",
        "    with open(output_file, 'wb') as f:\n",
        "        pickle.dump(embeddings, f)\n",
        "\n",
        "    print(f\"Embeddings saved to {output_file}\")\n",
        "\n",
        "# Example usage: Process IPC document and save embeddings\n",
        "process_ipc('/content/ipc_chunks_all.pkl', 'ipc_embeddings_st.pkl')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xjZ-ZTHOv9O5",
        "outputId": "e2d8cfa1-aef9-45b6-932f-764471c806ad"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Generating embeddings: 100%|██████████| 2369/2369 [02:24<00:00, 16.39it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Embeddings saved to ipc_embeddings_st.pkl\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Code to create Chunks**"
      ],
      "metadata": {
        "id": "zuBtt2suT2oC"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "6j27YgWVBGUl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import re\n",
        "import pickle\n",
        "from PyPDF2 import PdfReader\n",
        "\n",
        "# Function to read the IPC PDF and extract text\n",
        "def load_ipc_pdf(file_path):\n",
        "    text = \"\"\n",
        "    try:\n",
        "        reader = PdfReader(file_path)\n",
        "        for page in reader.pages:\n",
        "            text += page.extract_text() + \"\\n\"\n",
        "    except Exception as e:\n",
        "        print(f\"Error reading PDF file: {e}\")\n",
        "    return text\n",
        "\n",
        "# Function to split text into manageable chunks\n",
        "def split_text(text, max_chunk_size=512, min_chunk_size=256):\n",
        "    # Split at sentence boundaries while respecting chunk size constraints\n",
        "    sentences = re.split(r'(?<=\\.)\\s', text)\n",
        "    chunks = []\n",
        "    current_chunk = \"\"\n",
        "\n",
        "    for sentence in sentences:\n",
        "        if len(current_chunk) + len(sentence) <= max_chunk_size:\n",
        "            current_chunk += sentence + \" \"\n",
        "            # Ensure minimum chunk size is maintained\n",
        "            if len(current_chunk) >= min_chunk_size:\n",
        "                chunks.append(current_chunk.strip())\n",
        "                current_chunk = \"\"\n",
        "        else:\n",
        "            # If the current sentence exceeds max_chunk_size, split it forcibly\n",
        "            while len(sentence) > max_chunk_size:\n",
        "                chunks.append(sentence[:max_chunk_size].strip())\n",
        "                sentence = sentence[max_chunk_size:]\n",
        "            current_chunk = sentence + \" \"\n",
        "\n",
        "    # Append the remaining chunk if it meets the minimum size\n",
        "    if len(current_chunk) >= min_chunk_size:\n",
        "        chunks.append(current_chunk.strip())\n",
        "\n",
        "    return chunks\n",
        "\n",
        "# Main function to process the IPC documents in a folder and save chunks to a single pickle file\n",
        "def process_ipc_folder(folder_path, output_file):\n",
        "    all_chunks = []\n",
        "\n",
        "    # Iterate through all PDF files in the folder\n",
        "    for filename in os.listdir(folder_path):\n",
        "        if filename.endswith(\".pdf\"):\n",
        "            file_path = os.path.join(folder_path, filename)\n",
        "            print(f\"Processing file: {file_path}\")\n",
        "            # Load the IPC text and split into chunks\n",
        "            ipc_text = load_ipc_pdf(file_path)\n",
        "            ipc_chunks = split_text(ipc_text)\n",
        "            all_chunks.extend(ipc_chunks)\n",
        "\n",
        "    # Save all chunks to a pickle file\n",
        "    with open(output_file, 'wb') as f:\n",
        "        pickle.dump(all_chunks, f)\n",
        "\n",
        "# Example usage: Process all PDFs in '/content/Dataset' and save to 'ipc_chunks_all.pkl'\n",
        "process_ipc_folder('/content/Dataset', '/content/ipc_chunks_all.pkl')\n"
      ],
      "metadata": {
        "id": "dQ2EaHw5TlQc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f32f765c-7ea8-46ce-e817-eec85c211417"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing file: /content/Dataset/IPC_186045.pdf\n",
            "Processing file: /content/Dataset/A1860-45.pdf\n",
            "Processing file: /content/Dataset/1360312590693-12.Cyber-Laws-chapter-in-Legal-Aspects-Book.pdf\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Importing PKL files - end working model**"
      ],
      "metadata": {
        "id": "0fTpnusZx-un"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Deleted cause idk-"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2MavzxxFyFPq",
        "outputId": "247ecb28-317c-4fb7-a3b2-4f6309ebf5ab",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Result 1:\n",
            "Distance: 266.2577\n",
            "Text: 24 of 1995, Section 11.\n",
            "\n",
            "Result 2:\n",
            "Distance: 272.1407\n",
            "Text: 28 of 1993, section\n",
            "2.\n",
            "\n",
            "Result 3:\n",
            "Distance: 276.4108\n",
            "Text: [s 193] Punishment for false evidence.\n",
            "\n",
            "Result 4:\n",
            "Distance: 291.9615\n",
            "Text: [s 192] Fabricating false evidence.\n",
            "\n",
            "Result 5:\n",
            "Distance: 303.7898\n",
            "Text: This was an exception of the general rule\n",
            "of presumption of innocence of the accused.\n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'distance': 266.25766, 'text': '24 of 1995, Section 11.'},\n",
              " {'distance': 272.14072, 'text': '28 of 1993, section\\n2.'},\n",
              " {'distance': 276.4108, 'text': '[s 193] Punishment for false evidence.'},\n",
              " {'distance': 291.9615, 'text': '[s 192] Fabricating false evidence.'},\n",
              " {'distance': 303.78976,\n",
              "  'text': 'This was an exception of the general rule\\nof presumption of innocence of the accused.'}]"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **This is the final model, the one below is for experimentation. (it has a different output stance**"
      ],
      "metadata": {
        "id": "-JFPuABKP8li"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Further **Optimized**"
      ],
      "metadata": {
        "id": "CvOIs1guyM2D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModel\n",
        "import pickle\n",
        "import faiss\n",
        "import numpy as np\n",
        "\n",
        "# Load tokenizer and model for Legal-BERT\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"nlpaueb/legal-bert-base-uncased\")\n",
        "model = AutoModel.from_pretrained(\"nlpaueb/legal-bert-base-uncased\")\n",
        "\n",
        "# Check for GPU availability\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model.to(device)\n",
        "\n",
        "# Function to load chunks from a pickle file\n",
        "def load_chunks(file_path):\n",
        "    with open(file_path, 'rb') as f:\n",
        "        return pickle.load(f)\n",
        "\n",
        "# Function to calculate weighted query embedding\n",
        "def calculate_weighted_query_embedding(roberta_output):\n",
        "    embeddings = []\n",
        "    weights = []\n",
        "    for keyword, weight in roberta_output:\n",
        "        inputs = tokenizer(keyword, return_tensors='pt', truncation=True, padding=True).to(device)\n",
        "        with torch.no_grad():\n",
        "            outputs = model(**inputs)\n",
        "        embeddings.append(outputs.last_hidden_state[:, 0, :].cpu().numpy())\n",
        "        weights.append(weight)\n",
        "    embeddings = np.vstack(embeddings)\n",
        "    weights = np.array(weights).reshape(-1, 1)\n",
        "    weighted_embedding = np.sum(embeddings * weights, axis=0) / np.sum(weights)\n",
        "    return weighted_embedding\n",
        "\n",
        "# Main function to process the IPC document\n",
        "def process_ipc():\n",
        "    try:\n",
        "        # Load preprocessed chunks and embeddings\n",
        "        ipc_chunks = load_chunks('/content/ipc_chunks.pkl')\n",
        "        ipc_embeddings = load_chunks('/content/ipc_embeddings_st.pkl')\n",
        "        ipc_embeddings = np.vstack(ipc_embeddings)  # Ensure embeddings are a NumPy array\n",
        "    except (FileNotFoundError, EOFError):\n",
        "        print(\"Preprocessed chunks or embeddings not found. Please ensure the files exist.\")\n",
        "        return\n",
        "\n",
        "    # Initialize FAISS index\n",
        "    dimension = ipc_embeddings.shape[1]\n",
        "    index = faiss.IndexFlatL2(dimension)\n",
        "    index.add(ipc_embeddings.astype('float32'))  # Ensure ipc_embeddings are float32\n",
        "\n",
        "    # Example input from the RoBERTa model\n",
        "    roberta_output = [('7', 0.9539), ('ipc', 0.7163), ('section', 0.8221)]\n",
        "    query_embedding = calculate_weighted_query_embedding(roberta_output)\n",
        "\n",
        "    # Search in FAISS index\n",
        "    k = 5  # Number of top results to retrieve\n",
        "    query_embedding = query_embedding.reshape(1, -1).astype('float32')  # Ensure query_embedding is float32\n",
        "    distances, indices = index.search(query_embedding, k)\n",
        "\n",
        "    # Prepare and return results\n",
        "    results = []\n",
        "    for i, idx in enumerate(indices[0]):\n",
        "        result = {\n",
        "            \"distance\": distances[0][i],\n",
        "            \"text\": ipc_chunks[idx]\n",
        "        }\n",
        "        results.append(result)\n",
        "        print(f\"Result {i+1}:\")\n",
        "        print(f\"Distance: {result['distance']:.4f}\")\n",
        "        print(f\"Text: {result['text']}\\n\")\n",
        "\n",
        "    return results\n",
        "\n",
        "# Run the main processing function\n",
        "process_ipc()\n"
      ],
      "metadata": {
        "id": "OrCgZTy-yFjZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8a9d011e-afa9-45ec-f3e8-ed937c652d10"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Result 1:\n",
            "Distance: 131.8139\n",
            "Text: the State within \n",
            "which the offender is sentenced.]  \n",
            "56. [Sentence of Europeans and Americans to penal servitude. Proviso as to sentence for term \n",
            "exceeding ten years but not for life .] Rep. by the Criminal Law (Removal of Racial Discriminations) Act, \n",
            "1949 (1 7 of 1949) ( w.\n",
            "\n",
            "Result 2:\n",
            "Distance: 133.0580\n",
            "Text: e. f.  6-4-1949).  \n",
            "57. Fra ctions of terms of punishment .—In calculating fractions of terms  of punishment, \n",
            "2[impri sonment] for life shall be reckoned as equivalent to2[imprisonment] for twenty years.  \n",
            "58. [Offenders sentenced to transportation how dealt with until transported .] Rep.\n",
            "\n",
            "Result 3:\n",
            "Distance: 134.9268\n",
            "Text: by the Trade and Merchandise Marks Act , 1958 (43 of 1958),  s. 135 and \n",
            "Sch. (w. e. f.  25-11-1959).  \n",
            "479. Property mark .—A mark used for denoting that movable  property belongs to a particular \n",
            "person is called a property mark.  \n",
            "480. [Using a false trade mark .] Rep.\n",
            "\n",
            "Result 4:\n",
            "Distance: 135.3202\n",
            "Text: to the will, and appropriates them to his own use. A has comm itted criminal breach of trust.  \n",
            "(b) A is a warehouse -keeper. Z going on a journey, entrusts his furniture to A, under a contract that it shall be returned on \n",
            "payment of  a stipulated sum for wareh ouse room.\n",
            "\n",
            "Result 5:\n",
            "Distance: 135.3725\n",
            "Text: 3. Subs. by Act 3 of 1951, s. 3 and the Sch., for “the States”.  \n",
            "4. Subs. by Act 26 of  1955, s. 117 and the Sch., for “transportation for life” (w.e.f. 1 -1-1956). \n",
            "52 \n",
            " Illustratio n \n",
            "A gives false evidence before a Court of Justice, intending thereby to cause  Z to be convicted of a dacoity.\n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'distance': 131.81387,\n",
              "  'text': 'the State within \\nwhich the offender is sentenced.]  \\n56. [Sentence of Europeans and Americans to penal servitude. Proviso as to sentence for term \\nexceeding ten years but not for life .] Rep. by the Criminal Law (Removal of Racial Discriminations) Act, \\n1949 (1 7 of 1949) ( w.'},\n",
              " {'distance': 133.05804,\n",
              "  'text': 'e. f.  6-4-1949).  \\n57. Fra ctions of terms of punishment .—In calculating fractions of terms  of punishment, \\n2[impri sonment] for life shall be reckoned as equivalent to2[imprisonment] for twenty years.  \\n58. [Offenders sentenced to transportation how dealt with until transported .] Rep.'},\n",
              " {'distance': 134.92682,\n",
              "  'text': 'by the Trade and Merchandise Marks Act , 1958 (43 of 1958),  s. 135 and \\nSch. (w. e. f.  25-11-1959).  \\n479. Property mark .—A mark used for denoting that movable  property belongs to a particular \\nperson is called a property mark.  \\n480. [Using a false trade mark .] Rep.'},\n",
              " {'distance': 135.32018,\n",
              "  'text': 'to the will, and appropriates them to his own use. A has comm itted criminal breach of trust.  \\n(b) A is a warehouse -keeper. Z going on a journey, entrusts his furniture to A, under a contract that it shall be returned on \\npayment of  a stipulated sum for wareh ouse room.'},\n",
              " {'distance': 135.37254,\n",
              "  'text': '3. Subs. by Act 3 of 1951, s. 3 and the Sch., for “the States”.  \\n4. Subs. by Act 26 of  1955, s. 117 and the Sch., for “transportation for life” (w.e.f. 1 -1-1956). \\n52 \\n Illustratio n \\nA gives false evidence before a Court of Justice, intending thereby to cause  Z to be convicted of a dacoity.'}]"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "\n",
        "# Start the timer\n",
        "start = time.time()\n",
        "\n",
        "# Code to measure\n",
        "# Example function or process\n",
        "def example_process():\n",
        "    time.sleep(2)  # Simulate a process that takes 2 seconds\n",
        "\n",
        "example_process()\n",
        "\n",
        "# End the timer\n",
        "end = time.time()\n",
        "\n",
        "# Print the elapsed time\n",
        "print(\"Time taken: \", end - start, \"seconds\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fEP4pqiACczA",
        "outputId": "5e8879f4-1d25-4625-a2af-c1b449a262a7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Time taken:  2.0033864974975586 seconds\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Using MiniLM-L6-V2 model**"
      ],
      "metadata": {
        "id": "K7dVeJ5zy4W1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "import faiss\n",
        "import numpy as np\n",
        "from sentence_transformers import SentenceTransformer\n",
        "\n",
        "# Load pre-trained SentenceTransformer model\n",
        "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
        "\n",
        "# Function to load chunks from a pickle file\n",
        "def load_chunks(file_path):\n",
        "    with open(file_path, 'rb') as f:\n",
        "        return pickle.load(f)\n",
        "\n",
        "# Function to calculate weighted query embedding\n",
        "def calculate_weighted_query_embedding(roberta_output):\n",
        "    embeddings = []\n",
        "    weights = []\n",
        "    for keyword, weight in roberta_output:\n",
        "        embedding = model.encode(keyword, convert_to_tensor=True).cpu().numpy()\n",
        "        embeddings.append(embedding)\n",
        "        weights.append(weight)\n",
        "    embeddings = np.vstack(embeddings)\n",
        "    weights = np.array(weights).reshape(-1, 1)\n",
        "    weighted_embedding = np.sum(embeddings * weights, axis=0) / np.sum(weights)\n",
        "    return weighted_embedding\n",
        "\n",
        "# Main function to process the IPC document\n",
        "def process_ipc():\n",
        "    try:\n",
        "        # Load preprocessed chunks and embeddings\n",
        "        ipc_chunks = load_chunks('/content/ipc_chunks_all.pkl')\n",
        "        ipc_embeddings = load_chunks('/content/ipc_embeddings_st.pkl')\n",
        "        ipc_embeddings = np.vstack(ipc_embeddings)  # Ensure embeddings are a NumPy array\n",
        "    except (FileNotFoundError, EOFError):\n",
        "        print(\"Preprocessed chunks or embeddings not found. Please ensure the files exist.\")\n",
        "        return\n",
        "\n",
        "    # Initialize FAISS index\n",
        "    dimension = ipc_embeddings.shape[1]\n",
        "    index = faiss.IndexFlatL2(dimension)\n",
        "    index.add(ipc_embeddings.astype('float32'))  # Ensure ipc_embeddings are float32\n",
        "\n",
        "    # Example input from the RoBERTa model\n",
        "    roberta_output = [('69', 0.8539), ('ipc', 0.7163), ('section', 0.6221)]\n",
        "    query_embedding = calculate_weighted_query_embedding(roberta_output)\n",
        "\n",
        "    # Ensure the query_embedding has the same dimension as the FAISS index\n",
        "    assert query_embedding.shape[0] == dimension, f\"Dimension mismatch: query ({query_embedding.shape[0]}) vs index ({dimension})\"\n",
        "\n",
        "    # Search in FAISS index\n",
        "    k = 5  # Number of top results to retrieve\n",
        "    query_embedding = query_embedding.reshape(1, -1).astype('float32')  # Ensure query_embedding is float32\n",
        "    distances, indices = index.search(query_embedding, k)\n",
        "\n",
        "    # Prepare and return results\n",
        "    results = []\n",
        "    for i, idx in enumerate(indices[0]):\n",
        "        result = {\n",
        "            \"distance\": distances[0][i],\n",
        "            \"text\": ipc_chunks[idx]\n",
        "        }\n",
        "        results.append(result)\n",
        "        print(f\"Result {i+1}:\")\n",
        "        print(f\"Distance: {result['distance']:.4f}\")\n",
        "        print(f\"Text: {result['text']}\\n\")\n",
        "\n",
        "    return results\n",
        "\n",
        "# Run the main processing function\n",
        "process_ipc()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ynjpvcNQy3-i",
        "outputId": "3cebb9f3-cc08-4fc2-c303-e382fdfd589c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Result 1:\n",
            "Distance: 0.9252\n",
            "Text: Section 67-A deals with publishing or transmitting of material containi ng sexually explicit act in \n",
            "electronic form.  Contents of Section 67 when combined with  the material containing sexually explicit \n",
            "material attract penalty under this Section. \n",
            "Child Pornography  has been exclusively dealt with under Section 67B.\n",
            "\n",
            "Result 2:\n",
            "Distance: 0.9254\n",
            "Text: Section 69A inserted in the ITAA, vests with the Centra l Government or any of its officers \n",
            "with the powers to issue directions for blocking for publi c access of any information through \n",
            "any computer resource, under the same circumstances as me ntioned above.\n",
            "\n",
            "Result 3:\n",
            "Distance: 0.9610\n",
            "Text: 376C.  Sexual intercourse by a person in authority.  \n",
            "376D.  Gang rape . \n",
            "376E. Punishment for repeat offenders.  \n",
            "Of Unnatural offences  \n",
            "377. Unnatural offences.  \n",
            " \n",
            "CHAPTER XVII  \n",
            "OF OFFENCES AGAINST PROPERTY  \n",
            "Of Theft  \n",
            "378. Theft.  \n",
            "379. Punishment for theft.\n",
            "\n",
            "Result 4:\n",
            "Distance: 1.0146\n",
            "Text: 68 \n",
            " possession any obscene book, pamphlet, paper, drawing, painting, representation or figure or any \n",
            "other obscene object whatsoever, or  \n",
            "(b) imports, exports or conveys any obscene object for any of the purposes aforesaid, or knowing \n",
            "or having reason to believe that such object will be sold, let to hire, distributed or publicly exhibited \n",
            "or in any manner put into circu lation, or  \n",
            "(c) takes part in or receives profits from any business in the course of which he knows or has \n",
            "reason to believe that a\n",
            "\n",
            "Result 5:\n",
            "Distance: 1.0158\n",
            "Text: Ins. by Act 43 of 1983, s. 2.  \n",
            "2. Subs. by Act 13 of 2013, s. 4, for “offence under section 376, section 376A, section 376B, section  376C or section 376D” \n",
            "(w.e.f. 3 -2-2013 ). \n",
            "3. Subs. by Act 22 of 2018, s. 3, for “ section 376A, section 376B, section 376C, section 376D ” (w.e.f.\n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'distance': 0.9251923,\n",
              "  'text': 'Section 67-A deals with publishing or transmitting of material containi ng sexually explicit act in \\nelectronic form.  Contents of Section 67 when combined with  the material containing sexually explicit \\nmaterial attract penalty under this Section. \\nChild Pornography  has been exclusively dealt with under Section 67B.'},\n",
              " {'distance': 0.92542076,\n",
              "  'text': 'Section 69A inserted in the ITAA, vests with the Centra l Government or any of its officers \\nwith the powers to issue directions for blocking for publi c access of any information through \\nany computer resource, under the same circumstances as me ntioned above.'},\n",
              " {'distance': 0.9610262,\n",
              "  'text': '376C.  Sexual intercourse by a person in authority.  \\n376D.  Gang rape . \\n376E. Punishment for repeat offenders.  \\nOf Unnatural offences  \\n377. Unnatural offences.  \\n \\nCHAPTER XVII  \\nOF OFFENCES AGAINST PROPERTY  \\nOf Theft  \\n378. Theft.  \\n379. Punishment for theft.'},\n",
              " {'distance': 1.0146426,\n",
              "  'text': '68 \\n possession any obscene book, pamphlet, paper, drawing, painting, representation or figure or any \\nother obscene object whatsoever, or  \\n(b) imports, exports or conveys any obscene object for any of the purposes aforesaid, or knowing \\nor having reason to believe that such object will be sold, let to hire, distributed or publicly exhibited \\nor in any manner put into circu lation, or  \\n(c) takes part in or receives profits from any business in the course of which he knows or has \\nreason to believe that a'},\n",
              " {'distance': 1.0158297,\n",
              "  'text': 'Ins. by Act 43 of 1983, s. 2.  \\n2. Subs. by Act 13 of 2013, s. 4, for “offence under section 376, section 376A, section 376B, section  376C or section 376D” \\n(w.e.f. 3 -2-2013 ). \\n3. Subs. by Act 22 of 2018, s. 3, for “ section 376A, section 376B, section 376C, section 376D ” (w.e.f.'}]"
            ]
          },
          "metadata": {},
          "execution_count": 56
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Added Embedding Normalization**"
      ],
      "metadata": {
        "id": "yJGY9UzWx8jj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModel\n",
        "import pickle\n",
        "import faiss\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import normalize\n",
        "\n",
        "# Load tokenizer and model for Legal-BERT\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"nlpaueb/legal-bert-base-uncased\")\n",
        "model = AutoModel.from_pretrained(\"nlpaueb/legal-bert-base-uncased\")\n",
        "\n",
        "# Check for GPU availability\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model.to(device)\n",
        "\n",
        "# Function to load chunks from a pickle file\n",
        "def load_chunks(file_path):\n",
        "    with open(file_path, 'rb') as f:\n",
        "        return pickle.load(f)\n",
        "\n",
        "# Function to calculate weighted query embedding\n",
        "def calculate_weighted_query_embedding(roberta_output):\n",
        "    embeddings = []\n",
        "    weights = []\n",
        "    for keyword, weight in roberta_output:\n",
        "        inputs = tokenizer(keyword, return_tensors='pt', truncation=True, padding=True).to(device)\n",
        "        with torch.no_grad():\n",
        "            outputs = model(**inputs)\n",
        "        embeddings.append(outputs.last_hidden_state[:, 0, :].cpu().numpy())\n",
        "        weights.append(weight)\n",
        "    embeddings = np.vstack(embeddings)\n",
        "    weights = np.array(weights).reshape(-1, 1)\n",
        "    weighted_embedding = np.sum(embeddings * weights, axis=0) / np.sum(weights)\n",
        "    return weighted_embedding\n",
        "\n",
        "# Main function to process the IPC document\n",
        "def process_ipc():\n",
        "    try:\n",
        "        # Load preprocessed chunks and embeddings\n",
        "        ipc_chunks = load_chunks('/content/ipc_chunks.pkl')\n",
        "        ipc_embeddings = load_chunks('/content/ipc_embeddings.pkl')\n",
        "        ipc_embeddings = np.vstack(ipc_embeddings)  # Ensure embeddings are a NumPy array\n",
        "\n",
        "        # Normalize IPC embeddings\n",
        "        ipc_embeddings = normalize(ipc_embeddings, axis=1, norm='l2')\n",
        "\n",
        "    except (FileNotFoundError, EOFError):\n",
        "        print(\"Preprocessed chunks or embeddings not found. Please ensure the files exist.\")\n",
        "        return\n",
        "\n",
        "    # Initialize FAISS index\n",
        "    dimension = ipc_embeddings.shape[1]\n",
        "    index = faiss.IndexFlatL2(dimension)\n",
        "    index.add(ipc_embeddings.astype('float32'))  # Ensure ipc_embeddings are float32\n",
        "\n",
        "    # Example input from the RoBERTa model\n",
        "    roberta_output = [('146', 0.8539), ('ipc', 0.7163), ('section', 0.6221)]\n",
        "    query_embedding = calculate_weighted_query_embedding(roberta_output)\n",
        "\n",
        "    # Normalize query embedding\n",
        "    query_embedding = normalize(query_embedding.reshape(1, -1), norm='l2')\n",
        "\n",
        "    # Search in FAISS index\n",
        "    k = 5  # Number of top results to retrieve\n",
        "    query_embedding = query_embedding.astype('float32')  # Ensure query_embedding is float32\n",
        "    distances, indices = index.search(query_embedding, k)\n",
        "\n",
        "    # Prepare and return results\n",
        "    results = []\n",
        "    for i, idx in enumerate(indices[0]):\n",
        "        result = {\n",
        "            \"distance\": distances[0][i],\n",
        "            \"text\": ipc_chunks[idx]\n",
        "        }\n",
        "        results.append(result)\n",
        "        print(f\"Result {i+1}:\")\n",
        "        print(f\"Distance: {result['distance']:.4f}\")\n",
        "        print(f\"Text: {result['text']}\\n\")\n",
        "\n",
        "    return results\n",
        "\n",
        "# Run the main processing function\n",
        "process_ipc()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 338
        },
        "id": "8KE0uEAdSf-W",
        "outputId": "c8ec8a8a-a9dc-4884-d95a-5c15a6568c6f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AssertionError",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-18-757ca133989e>\u001b[0m in \u001b[0;36m<cell line: 83>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     81\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m \u001b[0;31m# Run the main processing function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 83\u001b[0;31m \u001b[0mprocess_ipc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-18-757ca133989e>\u001b[0m in \u001b[0;36mprocess_ipc\u001b[0;34m()\u001b[0m\n\u001b[1;32m     64\u001b[0m     \u001b[0mk\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m5\u001b[0m  \u001b[0;31m# Number of top results to retrieve\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m     \u001b[0mquery_embedding\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mquery_embedding\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'float32'\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Ensure query_embedding is float32\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 66\u001b[0;31m     \u001b[0mdistances\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msearch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquery_embedding\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     67\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m     \u001b[0;31m# Prepare and return results\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/faiss/__init__.py\u001b[0m in \u001b[0;36mreplacement_search\u001b[0;34m(self, x, k, D, I)\u001b[0m\n\u001b[1;32m    306\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    307\u001b[0m         \u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0md\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 308\u001b[0;31m         \u001b[0;32massert\u001b[0m \u001b[0md\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0md\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    309\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    310\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0mk\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAssertionError\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# FineTuning on IPC **CodeBook**"
      ],
      "metadata": {
        "id": "dxKwaY0-LUXj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install accelerate -U\n",
        "\n",
        "import accelerate\n",
        "print(accelerate.__version__)"
      ],
      "metadata": {
        "collapsed": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xZlsXLq_MgZR",
        "outputId": "2126359b-f6cb-4326-ebb1-b3d8d9ae0a5f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: accelerate in /usr/local/lib/python3.10/dist-packages (0.31.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from accelerate) (1.25.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from accelerate) (24.1)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate) (5.9.5)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from accelerate) (6.0.1)\n",
            "Requirement already satisfied: torch>=1.10.0 in /usr/local/lib/python3.10/dist-packages (from accelerate) (2.3.0+cu121)\n",
            "Requirement already satisfied: huggingface-hub in /usr/local/lib/python3.10/dist-packages (from accelerate) (0.23.4)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from accelerate) (0.4.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.15.3)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (4.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (1.12.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (2023.6.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (8.9.2.26)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (12.1.0.106)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.20.5 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (2.20.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (12.1.105)\n",
            "Requirement already satisfied: triton==2.3.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (2.3.0)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.10/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.10.0->accelerate) (12.5.40)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->accelerate) (2.31.0)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->accelerate) (4.66.4)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.10.0->accelerate) (2.1.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate) (2024.6.2)\n",
            "Requirement already satisfied: mpmath<1.4.0,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.10.0->accelerate) (1.3.0)\n",
            "0.31.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification, Trainer, TrainingArguments\n",
        "from tqdm import tqdm\n",
        "import pickle\n",
        "import numpy as np\n",
        "import os\n",
        "\n",
        "# Load tokenizer and model for Legal-BERT\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"nlpaueb/legal-bert-base-uncased\")\n",
        "model = AutoModelForSequenceClassification.from_pretrained(\"nlpaueb/legal-bert-base-uncased\")\n",
        "\n",
        "# Function to read the IPC PDF and extract text\n",
        "def load_ipc_pdf(file_path):\n",
        "    text = \"\"\n",
        "    try:\n",
        "        with open(file_path, 'rb') as f:\n",
        "            ipc_chunks = pickle.load(f)\n",
        "    except Exception as e:\n",
        "        print(f\"Error reading pkl file: {e}\")\n",
        "    return ipc_chunks\n",
        "\n",
        "# Function to fine-tune Legal-BERT on IPC chunks\n",
        "def fine_tune_legal_bert(ipc_chunks, labels):\n",
        "    # Tokenize IPC chunks\n",
        "    tokenized_data = tokenizer(ipc_chunks, truncation=True, padding=True, return_tensors='pt')\n",
        "\n",
        "\n",
        "    labels = [0] * len(ipc_chunks)\n",
        "\n",
        "    # Prepare training and validation sets\n",
        "    train_texts, val_texts, train_labels, val_labels = train_test_split(\n",
        "        tokenized_data['input_ids'], labels, random_state=42, test_size=0.2\n",
        "    )\n",
        "\n",
        "    # Define training arguments\n",
        "    training_args = TrainingArguments(\n",
        "        per_device_train_batch_size=4,\n",
        "        per_device_eval_batch_size=4,\n",
        "        num_train_epochs=3,\n",
        "        evaluation_strategy=\"epoch\",\n",
        "        logging_dir='./logs',\n",
        "        logging_steps=100,\n",
        "        output_dir='./output',\n",
        "    )\n",
        "\n",
        "    # Define Trainer\n",
        "    trainer = Trainer(\n",
        "        model=model,\n",
        "        args=training_args,\n",
        "        train_dataset=train_texts,\n",
        "        eval_dataset=val_texts,\n",
        "        compute_metrics=None,\n",
        "    )\n",
        "\n",
        "    # Fine-tune the model with tqdm progress bar\n",
        "    progress_bar = tqdm(range(training_args.num_train_epochs), desc=\"Training\")\n",
        "    for epoch in progress_bar:\n",
        "        trainer.train()\n",
        "        progress_bar.set_postfix({\"epoch\": epoch + 1})\n",
        "\n",
        "    # Save the fine-tuned model\n",
        "    output_model_dir = './fine_tuned_legal_bert'\n",
        "    model.save_pretrained(output_model_dir)\n",
        "    tokenizer.save_pretrained(output_model_dir)\n",
        "\n",
        "# Main function to process the IPC document and fine-tune Legal-BERT\n",
        "def process_ipc(file_path):\n",
        "    # Load the IPC chunks from pkl file\n",
        "    ipc_chunks = load_ipc_pdf(file_path)\n",
        "\n",
        "    # Fine-tune Legal-BERT on IPC chunks\n",
        "    fine_tune_legal_bert(ipc_chunks, labels=None)  # Replace with actual labels if available\n",
        "\n",
        "# Example usage: Process IPC document and fine-tune Legal-BERT\n",
        "process_ipc('/content/ipc_chunks.pkl')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "tjUkohznxzTl",
        "outputId": "2a44b4b0-2413-470a-ebed-a4db144dcb4e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at nlpaueb/legal-bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ImportError",
          "evalue": "Using the `Trainer` with `PyTorch` requires `accelerate>=0.21.0`: Please run `pip install transformers[torch]` or `pip install accelerate -U`",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-53-fcaa211b1b4f>\u001b[0m in \u001b[0;36m<cell line: 98>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     96\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m \u001b[0;31m# Example usage: Process IPC document and fine-tune Legal-BERT\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 98\u001b[0;31m \u001b[0mprocess_ipc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/Indian Penal Code Book.pdf'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-53-fcaa211b1b4f>\u001b[0m in \u001b[0;36mprocess_ipc\u001b[0;34m(file_path)\u001b[0m\n\u001b[1;32m     93\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m     \u001b[0;31m# Fine-tune Legal-BERT on IPC chunks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 95\u001b[0;31m     \u001b[0mfine_tune_legal_bert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mipc_chunks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     96\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m \u001b[0;31m# Example usage: Process IPC document and fine-tune Legal-BERT\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-53-fcaa211b1b4f>\u001b[0m in \u001b[0;36mfine_tune_legal_bert\u001b[0;34m(ipc_chunks, labels)\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m     \u001b[0;31m# Define training arguments\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 57\u001b[0;31m     training_args = TrainingArguments(\n\u001b[0m\u001b[1;32m     58\u001b[0m         \u001b[0moutput_dir\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_dir\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m         \u001b[0mper_device_train_batch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/training_args.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, output_dir, overwrite_output_dir, do_train, do_eval, do_predict, eval_strategy, prediction_loss_only, per_device_train_batch_size, per_device_eval_batch_size, per_gpu_train_batch_size, per_gpu_eval_batch_size, gradient_accumulation_steps, eval_accumulation_steps, eval_delay, learning_rate, weight_decay, adam_beta1, adam_beta2, adam_epsilon, max_grad_norm, num_train_epochs, max_steps, lr_scheduler_type, lr_scheduler_kwargs, warmup_ratio, warmup_steps, log_level, log_level_replica, log_on_each_node, logging_dir, logging_strategy, logging_first_step, logging_steps, logging_nan_inf_filter, save_strategy, save_steps, save_total_limit, save_safetensors, save_on_each_node, save_only_model, restore_callback_states_from_checkpoint, no_cuda, use_cpu, use_mps_device, seed, data_seed, jit_mode_eval, use_ipex, bf16, fp16, fp16_opt_level, half_precision_backend, bf16_full_eval, fp16_full_eval, tf32, local_rank, ddp_backend, tpu_num_cores, tpu_metrics_debug, debug, dataloader_drop_last, eval_steps, dataloader_num_workers, dataloader_prefetch_factor, past_index, run_name, disable_tqdm, remove_unused_columns, label_names, load_best_model_at_end, metric_for_best_model, greater_is_better, ignore_data_skip, fsdp, fsdp_min_num_params, fsdp_config, fsdp_transformer_layer_cls_to_wrap, accelerator_config, deepspeed, label_smoothing_factor, optim, optim_args, adafactor, group_by_length,...\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/training_args.py\u001b[0m in \u001b[0;36m__post_init__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1639\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mframework\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"pt\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1640\u001b[0m             \u001b[0;32mand\u001b[0m \u001b[0mis_torch_available\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1641\u001b[0;31m             \u001b[0;32mand\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtype\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"cpu\"\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mis_torch_greater_or_equal_than_2_3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1642\u001b[0m             \u001b[0;32mand\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtype\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m\"cuda\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1643\u001b[0m             \u001b[0;32mand\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtype\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m\"mlu\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/training_args.py\u001b[0m in \u001b[0;36mdevice\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   2147\u001b[0m         \"\"\"\n\u001b[1;32m   2148\u001b[0m         \u001b[0mrequires_backends\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m\"torch\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2149\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_setup_devices\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2150\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2151\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/utils/generic.py\u001b[0m in \u001b[0;36m__get__\u001b[0;34m(self, obj, objtype)\u001b[0m\n\u001b[1;32m     57\u001b[0m         \u001b[0mcached\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcached\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 59\u001b[0;31m             \u001b[0mcached\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     60\u001b[0m             \u001b[0msetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcached\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mcached\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/training_args.py\u001b[0m in \u001b[0;36m_setup_devices\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   2053\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mis_sagemaker_mp_enabled\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2054\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mis_accelerate_available\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2055\u001b[0;31m                 raise ImportError(\n\u001b[0m\u001b[1;32m   2056\u001b[0m                     \u001b[0;34mf\"Using the `Trainer` with `PyTorch` requires `accelerate>={ACCELERATE_MIN_VERSION}`: \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2057\u001b[0m                     \u001b[0;34m\"Please run `pip install transformers[torch]` or `pip install accelerate -U`\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mImportError\u001b[0m: Using the `Trainer` with `PyTorch` requires `accelerate>=0.21.0`: Please run `pip install transformers[torch]` or `pip install accelerate -U`",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}